{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection reduces the dimensionalit of data for the following reasons:\n",
    "- Reduces overfitting by removing noise introduced by some of the features\n",
    "- Reduces training time, which allows you to experiment more with different models and hyperparameters\n",
    "- Reduces data acquisition requirements\n",
    "- Improves comprehensibility of the model because a smaller set of features is more comprehendible to humans. This enables us to focus on the main sources of predictability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection methods generally fall into 2 categories. Filter Methods and Wrapper Methods. \n",
    "\n",
    "- Filter Methods: Apply a statistical measure and assign a score to each feature one at a time. Pearson's X2 and ANOVA F-Value based feature selection. \n",
    "\n",
    "- Wrapper Methods: Use a subset of features. Based on the results drawn from the previous model trained on that subset of features, they are either added or removed from the subset. The problem is essentially reduced to a search problem. Greedy algos (https://en.wikipedia.org/wiki/Greedy_algorithm) are the most desirable in multivariate feature selection scenarios because the wrapper methods are usually computationally very expensive and greedy algos don't necessarily provide the optimal solution, which is a good thing because it makes them less prone to overfitting. Forward Selection, Backward Elimination, Recursive Feature Elimination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.base import clone\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "datasource = \"datasets/winequality-red.csv\"\n",
    "print(os.path.exists(datasource))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.069</td>\n",
       "      <td>35.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.99632</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.081</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.99564</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.60</td>\n",
       "      <td>10.3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.2</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.081</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.00060</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.99750</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.61</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.7</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.112</td>\n",
       "      <td>11.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.99630</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.3             0.690         0.32             2.2      0.069   \n",
       "1            7.2             0.570         0.05             2.3      0.081   \n",
       "2           13.2             0.380         0.55             2.7      0.081   \n",
       "3            9.0             0.530         0.49             1.9      0.171   \n",
       "4            7.7             0.965         0.10             2.1      0.112   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 35.0                 104.0  0.99632  3.33       0.51   \n",
       "1                 16.0                  36.0  0.99564  3.38       0.60   \n",
       "2                  5.0                  16.0  1.00060  2.98       0.54   \n",
       "3                  6.0                  25.0  0.99750  3.27       0.61   \n",
       "4                 11.0                  22.0  0.99630  3.26       0.50   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.5        5  \n",
       "1     10.3        6  \n",
       "2      9.4        5  \n",
       "3      9.4        6  \n",
       "4      9.5        5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(datasource).sample(frac = 1).reset_index(drop = True)\n",
    "del df[\"Unnamed: 0\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(df.iloc[:, :-1])\n",
    "y = np.array(df[\"quality\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection solution space\n",
    "\n",
    "From algorithm analysus' point of view, a solution for feature selection problems can be represented as a boolean vector, each component indicating whether the corresponding feature has been selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True False False  True  True False False False  True]\n"
     ]
    }
   ],
   "source": [
    "selected = np.array([False, True, True, False, False, True, True, False, False, False, True])\n",
    "print(selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn calls the corresponding indices to feature columns selected \"support\", which can be obtained using np.flatnonzero(). \n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.flatnonzero.html\n",
    "\n",
    "Return indices that are non-zero in the flattened version of a.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  5  6 10]\n"
     ]
    }
   ],
   "source": [
    "support = np.flatnonzero(selected)\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, a naive approach that exhaustively search all subsets of features would have to verify 2^p solutions. This is very inefficient. However, we will run an exhaustive search for all solutions that provide 5 features to establish a baseline. This limits our time complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_combinations(estimator, X, y, k = 5):\n",
    "    # fit and score model based on some subset of features\n",
    "    score = lambda X_features: clone(estimator).fit(X_features, y).score(X_features, y)\n",
    "    \n",
    "    # enumerate all combinations of 5 features\n",
    "    for subset in itertools.combinations(range(X.shape[1]), 5):\n",
    "        yield score(X[:, subset]), subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = search_combinations(LinearRegression(), X, y)\n",
    "\n",
    "# feed it a model, X, and y. \n",
    "# it'll iterate through all possible variations (up to 5)\n",
    "# and fit/score on those variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.35149423850184036, (1, 4, 6, 9, 10)),\n",
       " (0.34831403567393349, (1, 4, 8, 9, 10)),\n",
       " (0.34695788821029638, (1, 6, 8, 9, 10)),\n",
       " (0.34667490118495603, (0, 1, 4, 9, 10)),\n",
       " (0.34580097696162615, (0, 1, 6, 9, 10))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(scores, reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Selection\n",
    "\n",
    "Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_select(estimator, X, y, k = 2):\n",
    "    # this array holds indicators of whether each feature is currently selected\n",
    "    selected = np.zeros(X.shape[1]).astype(bool)\n",
    "    \n",
    "    # fit and score the model based on some subset of features\n",
    "    score = lambda X_features: clone(estimator).fit(X_features, y).score(X_features, y)\n",
    "    \n",
    "    # find indices to selected columns\n",
    "    selected_indices = lambda: list(np.flatnonzero(selected))\n",
    "    \n",
    "    # repeated til k features are selected\n",
    "    while np.sum(selected) < k:\n",
    "        # indices to unselected column\n",
    "        rest_indices = list(np.flatnonzero(~selected))\n",
    "        \n",
    "        # compute model scores with an additional feature\n",
    "        scores = [score(X[:, selected_indices() + [i]]) for i in rest_indices]\n",
    "        \n",
    "        print(\"\\n * accuracy if adding column: \\n    \", {i: int(s * 100) for i, s in zip(rest_indices, scores)})\n",
    "        \n",
    "        # find index within \"rest_indices\" that points to the most predictive feature not yet selected\n",
    "        idx_to_add = rest_indices[np.argmax(scores)]\n",
    "        print(\"add column\", idx_to_add)\n",
    "        \n",
    "        # select this new feature\n",
    "        selected[idx_to_add] = True\n",
    "        print(\"================================\")\n",
    "        \n",
    "    return selected_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * accuracy if adding column: \n",
      "     {0: 1, 1: 15, 2: 5, 3: 0, 4: 1, 5: 0, 6: 3, 7: 3, 8: 0, 9: 6, 10: 22}\n",
      "add column 10\n",
      "================================\n",
      "\n",
      " * accuracy if adding column: \n",
      "     {0: 25, 1: 31, 2: 25, 3: 22, 4: 22, 5: 22, 6: 23, 7: 23, 8: 25, 9: 26}\n",
      "add column 1\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "support = sorted(forward_select(LinearRegression(), X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backwards Elimination\n",
    "\n",
    "In backwards elimination, we basically just do the opposite. We start with ALL the features and remove the least significant feature at each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backwards_eliminate(estimator, X, y, k = 5):\n",
    "    # this array holds indicators of whether each feature is currently selected\n",
    "    selected = np.ones(X.shape[1]).astype(bool)\n",
    "    \n",
    "    # fit and score model based on some subset of features\n",
    "    score = lambda X_features: clone(estimator).fit(X_features, y).score(X_features, y)\n",
    "    \n",
    "    # find indices to selected columns\n",
    "    selected_indices = lambda: list(np.flatnonzero(selected))\n",
    "    \n",
    "    # repeat til k features are selected\n",
    "    while np.sum(selected) > k:\n",
    "        # compute model scores with one of the features removed\n",
    "        scores = [score(X[:, list(set(selected_indices()) - {i})]) for i in selected_indices()]\n",
    "        print(\"\\n accuracy if removing column: \\n\", {i: int(s*100) for i, s in zip(selected_indices(), scores)})\n",
    "        \n",
    "        # find index that points to the least predictive feature\n",
    "        idx_to_remove = selected_indices()[np.argmax(scores)]\n",
    "        print(\"remove column\", idx_to_remove)\n",
    "        \n",
    "        # remove this feature\n",
    "        selected[idx_to_remove] = False\n",
    "        print(\"================================\")\n",
    "        \n",
    "    return selected_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " accuracy if removing column: \n",
      " {0: 36, 1: 32, 2: 35, 3: 36, 4: 35, 5: 35, 6: 35, 7: 36, 8: 35, 9: 33, 10: 31}\n",
      "remove column 7\n",
      "================================\n",
      "\n",
      " accuracy if removing column: \n",
      " {0: 36, 1: 32, 2: 35, 3: 36, 4: 35, 5: 35, 6: 35, 8: 35, 9: 33, 10: 24}\n",
      "remove column 0\n",
      "================================\n",
      "\n",
      " accuracy if removing column: \n",
      " {1: 32, 2: 35, 3: 35, 4: 35, 5: 35, 6: 35, 8: 35, 9: 33, 10: 24}\n",
      "remove column 3\n",
      "================================\n",
      "\n",
      " accuracy if removing column: \n",
      " {1: 32, 2: 35, 4: 35, 5: 35, 6: 35, 8: 35, 9: 33, 10: 24}\n",
      "remove column 2\n",
      "================================\n",
      "\n",
      " accuracy if removing column: \n",
      " {1: 31, 4: 34, 5: 35, 6: 34, 8: 35, 9: 33, 10: 24}\n",
      "remove column 5\n",
      "================================\n",
      "\n",
      " accuracy if removing column: \n",
      " {1: 31, 4: 34, 6: 34, 8: 35, 9: 33, 10: 23}\n",
      "remove column 8\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "support = sorted(backwards_eliminate(LinearRegression(), X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "Recursive feature elimination is an even more greedy algo. It finds good performing feature subset with high efficiency. The importance of each feature is obtained either through a coef attribute or through a feature_importances attribute. So in order for recursive feature elimination to work, the model is required to provide either of these attributes. \n",
    "\n",
    "We typically start off using a low complexity model and use it as a benchmark for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFE(estimator=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       "  n_features_to_select=5, step=1, verbose=0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "selector = RFE(model, 5)\n",
    "selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Features:\", selector.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: [1 4 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(\"Selected Features:\", np.flatnonzero(selector.support_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we transform the dataset to include only these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 5)\n"
     ]
    }
   ],
   "source": [
    "X_new = selector.transform(X)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Methods\n",
    "\n",
    "These methods rank feature predictiveness one by one, as opposed to considering a subset. They incorporate statistical methods to rank each feature instead of measuring accuracy of a model trained on selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
