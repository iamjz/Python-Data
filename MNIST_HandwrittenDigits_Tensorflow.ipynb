{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST handwritten digits\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os, sys\n",
    "import itertools, functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import scale, LabelBinarizer\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(18937)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "Keras is an abstraction of the tensorflow api to facilitate more easily constructed models. And actually, it is a general python library for model construction that support tensorflow and some other underlying libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dense, Dropout, Reshape\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://keras.io/\n",
    "\n",
    "We'll construct a convolutional neural network (http://deeplearning.net/tutorial/lenet.html) that has the following structure:\n",
    "* Convolution with 5x5 pixel kernels, 32 of them, and using the Recitified Linear Unit (https://en.wikipedia.org/wiki/Rectifier_(neural_networks))\n",
    "* Max pooling with 2x2 kernel. Find the strongest response in each 2x2 neuron area of a generated feature map (from the convolution)\n",
    "* Convolve with 64 5x5 kernels, then max pooling again\n",
    "* Stretch all the feature maps out into a vector\n",
    "* A feed forward, fully connected layer\n",
    "* 10 class activation using SoftMax, a logit layer, with all neurons normalized to sum to 1.0 (https://en.wikipedia.org/wiki/Softmax_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              3212288   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 3,274,634\n",
      "Trainable params: 3,274,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    Input(shape=(784, )),\n",
    "    Reshape((28, 28, 1)),\n",
    "    \n",
    "    # convolutional network\n",
    "    Conv2D(32, (5, 5), activation = \"relu\", padding = \"SAME\"),\n",
    "    MaxPooling2D((2, 2), strides = (2, 2), padding = \"SAME\"),\n",
    "    Conv2D(64, (5, 5), activation = \"relu\", padding = \"SAME\"), \n",
    "    MaxPooling2D((2, 2), strides = (2, 2), padding = \"SAME\"),\n",
    "    \n",
    "    # fully connected network\n",
    "    Reshape((7*7*64,)),\n",
    "    Dense(1024, activation = \"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation = \"softmax\"),\n",
    "]\n",
    "\n",
    "y_pred = functools.reduce(lambda f1, f2: f2(f1), layers)\n",
    "\n",
    "model = Model(inputs = [layers[0]], outputs = [y_pred])\n",
    "model.compile(optimizer = Adam(), loss = \"categorical_crossentropy\", metrics = [\"categorical_accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting datasets/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials import mnist\n",
    "dataset = mnist.input_data.read_data_sets(\"datasets/MNIST_data\", one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 316s - loss: 0.1282 - categorical_accuracy: 0.9612 - val_loss: 0.0409 - val_categorical_accuracy: 0.9860\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 294s - loss: 0.0409 - categorical_accuracy: 0.9871 - val_loss: 0.0342 - val_categorical_accuracy: 0.9902\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 347s - loss: 0.0298 - categorical_accuracy: 0.9908 - val_loss: 0.0326 - val_categorical_accuracy: 0.9898\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 335s - loss: 0.0231 - categorical_accuracy: 0.9929 - val_loss: 0.0458 - val_categorical_accuracy: 0.9860\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 356s - loss: 0.0196 - categorical_accuracy: 0.9939 - val_loss: 0.0388 - val_categorical_accuracy: 0.9912\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 321s - loss: 0.0155 - categorical_accuracy: 0.9953 - val_loss: 0.0338 - val_categorical_accuracy: 0.9908\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 331s - loss: 0.0143 - categorical_accuracy: 0.9953 - val_loss: 0.0296 - val_categorical_accuracy: 0.9912\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 316s - loss: 0.0106 - categorical_accuracy: 0.9968 - val_loss: 0.0360 - val_categorical_accuracy: 0.9926\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 283s - loss: 0.0112 - categorical_accuracy: 0.9966 - val_loss: 0.0321 - val_categorical_accuracy: 0.9926\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 305s - loss: 0.0090 - categorical_accuracy: 0.9972 - val_loss: 0.0338 - val_categorical_accuracy: 0.9930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20580510438>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = [dataset.train.images], y = [dataset.train.labels], batch_size = 50, epochs = 10, \n",
    "         validation_data = (dataset.validation.images, dataset.validation.labels), shuffle = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: \n",
      "\n",
      "loss: 0.0314 accuracy: 0.9918\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation: \\n\")\n",
    "print(\"loss: %.4f accuracy: %.4f\" %\n",
    "         tuple(model.evaluate(x = [dataset.test.images], y = [dataset.test.labels], batch_size = 50, verbose = 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
