{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing is an important step in the use of unstructured text document for any type of data mining, information retrieval, or text analytics. We'll use NLTK and look at the following concepts:\n",
    "* Stop Words\n",
    "* Stemming\n",
    "* Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint, string, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "Text documents often contain many occurences of the same word. For example, words such as \"a\", \"the\", \"of\", and \"it\" likely occur very frequently. When classifying a document based on the number of times specific words occur in the text document, these words can lead to biases, especially since they are generally common in ALL text documents you might want to classify. \n",
    "\n",
    "As a result, the concept of stop words was invented. \n",
    "\n",
    "https://en.wikipedia.org/wiki/Stop_words\n",
    "\n",
    "These words are the most commonly occurring words and they should be removed during the tokenization process in order to improve subseqeunt text analytics efforts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer = \"word\", lowercase = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myText = \"Hello! My name is Jacky Zhao. I'm an aspiring data scientist. Follow me on twitter @iamdatabear. The is my text analysis practice!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! My name is Jacky Zhao. I'm an aspiring data scientist. Follow me on twitter @iamdatabear. The is my text analysis practice!\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(lowercase = True)\n",
    "cv2 = CountVectorizer(stop_words = \"english\", lowercase = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tk_func1 = cv1.build_analyzer()\n",
    "tk_func2 = cv2.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent = 2, depth = 1, width = 80, compact = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization for 1: \n",
      " ['hello', 'my', 'name', 'is', 'jacky', 'zhao', 'an', 'aspiring', 'data', 'scientist', 'follow', 'me', 'on', 'twitter', 'iamdatabear', 'the', 'is', 'my', 'text', 'analysis', 'practice']\n"
     ]
    }
   ],
   "source": [
    "example1 = tk_func1(myText)\n",
    "print(\"Tokenization for 1: \\n\", example1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization for 2: \n",
      " ['hello', 'jacky', 'zhao', 'aspiring', 'data', 'scientist', 'follow', 'twitter', 'iamdatabear', 'text', 'analysis', 'practice']\n"
     ]
    }
   ],
   "source": [
    "example2 = tk_func2(myText)\n",
    "print(\"Tokenization for 2: \\n\", example2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "We have looked at the removal of redundant or unimportant words (stop words). However, an issue still exist because of different word forms of the same base term. For example: compute, computer, computed, and computing. The process of changing words back to their root term (basic term) so that token frequencies match the use of the root token rather than being spread across multiple tokens is known as stemming\n",
    "\n",
    "https://en.wikipedia.org/wiki/Stemming\n",
    "\n",
    "The most popular stemming is the \"Porter Stemmer\". It was originally published by Martin Porterin 1980. Since then, an improved version was released in 2000. NLTK includes the Porter Stemmer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_words = [\"python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "for w in example_words:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newText = \"It is important to be very pythonly while you are pythoning with pythong. All pythoners have pythoned poorly at least once!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is important to be very pythonly while you are pythoning with pythong. All pythoners have pythoned poorly at least once!'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(newText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'important', 'to', 'be', 'very', 'pythonly', 'while', 'you', 'are', 'pythoning', 'with', 'pythong', '.', 'All', 'pythoners', 'have', 'pythoned', 'poorly', 'at', 'least', 'once', '!']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [token for token in tokens if token not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'important', 'to', 'be', 'very', 'pythonly', 'while', 'you', 'are', 'pythoning', 'with', 'pythong', 'All', 'pythoners', 'have', 'pythoned', 'poorly', 'at', 'least', 'once']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "is\n",
      "import\n",
      "to\n",
      "be\n",
      "veri\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "pythong\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n"
     ]
    }
   ],
   "source": [
    "for w in tokens:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Lemmatization in linguistics is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. By inflected, it means to change the form of a word to express a particular grammatical function or attribute, typically tense, mood, person, number, case, and gender.\n",
    "\n",
    "In computational linguistics, lemmatization is the algorithmic process of determining the lemma for a given word. The process may involve complex tasks such as understanding the context and determining the parts of speech of a word. \n",
    "\n",
    "In many languages, words appear in several inflected forms. For example, the verb \"to walk\" may appear as \"walk\", \"walked\", \"walks\", and \"walking\". The base form \"walk\" is called the lemma of the word. \n",
    "\n",
    "Lemmatization is closely related to stemming. The difference is that a stemmer operates on a single word without the knowledge of the context and therefore cannot discriminate between words which have different meanings depending on the part of speech. However, stemmers are typically easier to implement and run much faster. The reduced accuracy may not matter for some applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
