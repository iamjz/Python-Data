{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection reduces the dimensionalit of data for the following reasons:\n",
    "- Reduces overfitting by removing noise introduced by some of the features\n",
    "- Reduces training time, which allows you to experiment more with different models and hyperparameters\n",
    "- Reduces data acquisition requirements\n",
    "- Improves comprehensibility of the model because a smaller set of features is more comprehendible to humans. This enables us to focus on the main sources of predictability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection methods generally fall into 2 categories. Filter Methods and Wrapper Methods. \n",
    "\n",
    "- Filter Methods: Apply a statistical measure and assign a score to each feature one at a time. Pearson's X2 and ANOVA F-Value based feature selection. \n",
    "\n",
    "- Wrapper Methods: Use a subset of features. Based on the results drawn from the previous model trained on that subset of features, they are either added or removed from the subset. The problem is essentially reduced to a search problem. Greedy algos (https://en.wikipedia.org/wiki/Greedy_algorithm) are the most desirable in multivariate feature selection scenarios because the wrapper methods are usually computationally very expensive and greedy algos don't necessarily provide the optimal solution, which is a good thing because it makes them less prone to overfitting. Forward Selection, Backward Elimination, Recursive Feature Elimination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.base import clone\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "datasource = \"datasets/winequality-red.csv\"\n",
    "print(os.path.exists(datasource))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.082</td>\n",
       "      <td>23.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.99820</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.23</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.111</td>\n",
       "      <td>20.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.99640</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.61</td>\n",
       "      <td>9.3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.078</td>\n",
       "      <td>19.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.99612</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.48</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.071</td>\n",
       "      <td>7.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.99640</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.55</td>\n",
       "      <td>9.7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.7</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.069</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.99458</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.6             0.390         0.31             2.3      0.082   \n",
       "1            7.6             0.685         0.23             2.3      0.111   \n",
       "2            8.0             0.520         0.25             2.0      0.078   \n",
       "3            7.6             0.550         0.21             2.2      0.071   \n",
       "4            7.7             0.570         0.21             1.5      0.069   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 23.0                  71.0  0.99820  3.52       0.65   \n",
       "1                 20.0                  84.0  0.99640  3.21       0.61   \n",
       "2                 19.0                  59.0  0.99612  3.30       0.48   \n",
       "3                  7.0                  28.0  0.99640  3.28       0.55   \n",
       "4                  4.0                   9.0  0.99458  3.16       0.54   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.7        5  \n",
       "1      9.3        5  \n",
       "2     10.2        5  \n",
       "3      9.7        5  \n",
       "4      9.8        6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(datasource).sample(frac = 1).reset_index(drop = True)\n",
    "del df[\"Unnamed: 0\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(df.iloc[:, :-1])\n",
    "y = np.array(df[\"quality\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection solution space\n",
    "\n",
    "From algorithm analysus' point of view, a solution for feature selection problems can be represented as a boolean vector, each component indicating whether the corresponding feature has been selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True False False  True  True False False False  True]\n"
     ]
    }
   ],
   "source": [
    "selected = np.array([False, True, True, False, False, True, True, False, False, False, True])\n",
    "print(selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn calls the corresponding indices to feature columns selected \"support\", which can be obtained using np.flatnonzero(). \n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.flatnonzero.html\n",
    "\n",
    "Return indices that are non-zero in the flattened version of a.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  5  6 10]\n"
     ]
    }
   ],
   "source": [
    "support = np.flatnonzero(selected)\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, a naive approach that exhaustively search all subsets of features would have to verify 2^p solutions. This is very inefficient. However, we will run an exhaustive search for all solutions that provide 5 features to establish a baseline. This limits our time complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_combinations(estimator, X, y, k = 5):\n",
    "    # fit and score model based on some subset of features\n",
    "    score = lambda X_features: clone(estimator).fit(X_features, y).score(X_features, y)\n",
    "    \n",
    "    # enumerate all combinations of 5 features\n",
    "    for subset in itertools.combinations(range(X.shape[1]), 5):\n",
    "        yield score(X[:, subset]), subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = search_combinations(LinearRegression(), X, y)\n",
    "\n",
    "# feed it a model, X, and y. \n",
    "# it'll iterate through all possible variations (up to 5)\n",
    "# and fit/score on those variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.35149423850184036, (1, 4, 6, 9, 10)),\n",
       " (0.34831403567393349, (1, 4, 8, 9, 10)),\n",
       " (0.34695788821029638, (1, 6, 8, 9, 10)),\n",
       " (0.34667490118495603, (0, 1, 4, 9, 10)),\n",
       " (0.34580097696162626, (0, 1, 6, 9, 10))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(scores, reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Selection\n",
    "\n",
    "Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_select(estimator, X, y, k = 2):\n",
    "    # this array holds indicators of whether each feature is currently selected\n",
    "    selected = np.zeros(X.shape[1]).astype(bool)\n",
    "    \n",
    "    # fit and score the model based on some subset of features\n",
    "    score = lambda X_features: clone(estimator).fit(X_features, y).score(X_features, y)\n",
    "    \n",
    "    # find indices to selected columns\n",
    "    selected_indices = lambda: list(np.flatnonzero(selected))\n",
    "    \n",
    "    # repeated til k features are selected\n",
    "    while np.sum(selected) < k:\n",
    "        # indices to unselected column\n",
    "        rest_indices = list(np.flatnonzero(~selected))\n",
    "        \n",
    "        # compute model scores with an additional feature\n",
    "        scores = [score(X[:, selected_indices() + [i]]) for i in rest_indices]\n",
    "        \n",
    "        print(\"\\n * accuracy if adding column: \\n    \", {i: int(s * 100) for i, s in zip(rest_indices, scores)})\n",
    "        \n",
    "        # find index within \"rest_indices\" that points to the most predictive feature not yet selected\n",
    "        idx_to_add = rest_indices[np.argmax(scores)]\n",
    "        print(\"add column\", idx_to_add)\n",
    "        \n",
    "        # select this new feature\n",
    "        selected[idx_to_add] = True\n",
    "        print(\"================================\")\n",
    "        \n",
    "    return selected_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * accuracy if adding column: \n",
      "     {0: 1, 1: 15, 2: 5, 3: 0, 4: 1, 5: 0, 6: 3, 7: 3, 8: 0, 9: 6, 10: 22}\n",
      "add column 10\n",
      "================================\n",
      "\n",
      " * accuracy if adding column: \n",
      "     {0: 25, 1: 31, 2: 25, 3: 22, 4: 22, 5: 22, 6: 23, 7: 23, 8: 25, 9: 26}\n",
      "add column 1\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "support = sorted(forward_select(LinearRegression(), X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backwards Elimination\n",
    "\n",
    "In backwards elimination, we basically just do the opposite. We start with ALL the features and remove the least significant feature at each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backwards_eliminate(estimator, X, y, k = 5):\n",
    "    # this array holds indicators of whether each feature is currently selected\n",
    "    selected = np.ones(X.shape[1]).astype(bool)\n",
    "    \n",
    "    # fit and score model based on some subset of features\n",
    "    score = lambda X_features: clone(estimator).fit(X_features, y).score(X_features, y)\n",
    "    \n",
    "    # find indices to selected columns\n",
    "    selected_indices = lambda: list(np.flatnonzero(selected))\n",
    "    \n",
    "    # repeat til k features are selected\n",
    "    while np.sum(selected) > k:\n",
    "        # compute model scores with one of the features removed\n",
    "        scores = [score(X[:, list(set(selected_indices()) - {i})]) for i in selected_indices()]\n",
    "        print(\"\\n accuracy if removing column: \\n\", {i: int(s*100) for i, s in zip(selected_indices(), scores)})\n",
    "        \n",
    "        # find index that points to the least predictive feature\n",
    "        idx_to_remove = selected_indices()[np.argmax(scores)]\n",
    "        print(\"remove column\", idx_to_remove)\n",
    "        \n",
    "        # remove this feature\n",
    "        selected[idx_to_remove] = False\n",
    "        print(\"================================\")\n",
    "        \n",
    "    return selected_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " accuracy if removing column: \n",
      " {0: 36, 1: 32, 2: 35, 3: 36, 4: 35, 5: 35, 6: 35, 7: 36, 8: 35, 9: 33, 10: 31}\n",
      "remove column 7\n",
      "================================\n",
      "\n",
      " accuracy if removing column: \n",
      " {0: 36, 1: 32, 2: 35, 3: 36, 4: 35, 5: 35, 6: 35, 8: 35, 9: 33, 10: 24}\n",
      "remove column 0\n",
      "================================\n",
      "\n",
      " accuracy if removing column: \n",
      " {1: 32, 2: 35, 3: 35, 4: 35, 5: 35, 6: 35, 8: 35, 9: 33, 10: 24}\n",
      "remove column 3\n",
      "================================\n",
      "\n",
      " accuracy if removing column: \n",
      " {1: 32, 2: 35, 4: 35, 5: 35, 6: 35, 8: 35, 9: 33, 10: 24}\n",
      "remove column 2\n",
      "================================\n",
      "\n",
      " accuracy if removing column: \n",
      " {1: 31, 4: 34, 5: 35, 6: 34, 8: 35, 9: 33, 10: 24}\n",
      "remove column 5\n",
      "================================\n",
      "\n",
      " accuracy if removing column: \n",
      " {1: 31, 4: 34, 6: 34, 8: 35, 9: 33, 10: 23}\n",
      "remove column 8\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "support = sorted(backwards_eliminate(LinearRegression(), X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "Recursive feature elimination is an even more greedy algo. It finds good performing feature subset with high efficiency. The importance of each feature is obtained either through a coef attribute or through a feature_importances attribute. So in order for recursive feature elimination to work, the model is required to provide either of these attributes. \n",
    "\n",
    "We typically start off using a low complexity model and use it as a benchmark for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFE(estimator=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       "  n_features_to_select=5, step=1, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "selector = RFE(model, 5)\n",
    "selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Features:\", selector.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: [1 4 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(\"Selected Features:\", np.flatnonzero(selector.support_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we transform the dataset to include only these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 5)\n"
     ]
    }
   ],
   "source": [
    "X_new = selector.transform(X)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Methods\n",
    "\n",
    "These methods rank feature predictiveness one by one, as opposed to considering a subset. They incorporate statistical methods to rank each feature instead of measuring accuracy of a model trained on selected features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Peason's X^2 test based feature selection\n",
    "The following constructs the approximate X^2 distribution and scores each feature vs the label in order to determine which feature is more relevant, one at a time...and then selects features according to the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2, k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=5, score_func=<function chi2 at 0x00000181D77B2158>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X^2 statistic: \n",
      " [  1.12606524e+01   1.55802891e+01   1.30256651e+01   4.12329474e+00\n",
      "   7.52425579e-01   1.61936036e+02   2.75555798e+03   2.30432045e-04\n",
      "   1.54654736e-01   4.55848775e+00   4.64298922e+01]\n"
     ]
    }
   ],
   "source": [
    "print(\"X^2 statistic: \\n\", selector.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected indices: \n",
      " [ 1  2  5  6 10]\n"
     ]
    }
   ],
   "source": [
    "print(\"Selected indices: \\n\", selector.get_support(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then call transform() to select those feature columns from the dataset and store them into a new variable called X_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_selected = selector.transform(X)\n",
    "X_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can slice and dice. This does the same thing as transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(X_selected, X[:, [1, 2, 5, 6, 10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.39 ,  0.31 ],\n",
       "       [ 0.685,  0.23 ],\n",
       "       [ 0.52 ,  0.25 ],\n",
       "       ..., \n",
       "       [ 0.64 ,  0.07 ],\n",
       "       [ 0.43 ,  0.29 ],\n",
       "       [ 0.58 ,  0.54 ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_selected[:, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at this procedure so we can better distinguish different feature selection methods. X^2 test has many applications. For feature selection, we utilize X^2 statistic to test for dependence of each feature towards determining label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Encode labels into orthogonal vector space\n",
    "This is also known as one-hot encoding. It's applicable to classification problems. Consider an example where you have defined 3 categories for possible outcomes: A/B/C. In order for machine learning algos to be able to handle this type of data, we have to convert them to numbers. One-hot encoding uses a vector (y1, y2, y3) where yi = [result falls into the ith category]. \n",
    "\n",
    "Therefore, one-hot encoding for A, B, and C categories becomes (1, 0, 0), (0, 1, 0), and (0, 0, 1) respectively. This has an advantage over plainly translating A, B, C into 1, 2, 3 (aka sparse encoding) in a way that orthogonal vectors do not impose assumptions of their order or magnitudes between categories like numbers would. \n",
    "\n",
    "For example, 3 > 1 is true. However, it doesn't mean to imply that C > A or C is superior to A in any way. However, this would affect the model's numerical stability. \n",
    "\n",
    "Therefore, one-hot encoding is a widely adopted technique for processing categories. Sparse encoding could be used when persisting a dataset in order to save storage space. One-hot encoding is how the categorical/nominal variables are encoded as independent predictors in the regression formula. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
