{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection reduces the dimensionalit of data for the following reasons:\n",
    "- Reduces overfitting by removing noise introduced by some of the features\n",
    "- Reduces training time, which allows you to experiment more with different models and hyperparameters\n",
    "- Reduces data acquisition requirements\n",
    "- Improves comprehensibility of the model because a smaller set of features is more comprehendible to humans. This enables us to focus on the main sources of predictability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection methods generally fall into 2 categories. Filter Methods and Wrapper Methods. \n",
    "\n",
    "- Filter Methods: Apply a statistical measure and assign a score to each feature one at a time. Pearson's X2 and ANOVA F-Value based feature selection. \n",
    "\n",
    "- Wrapper Methods: Use a subset of features. Based on the results drawn from the previous model trained on that subset of features, they are either added or removed from the subset. The problem is essentially reduced to a search problem. Greedy algos (https://en.wikipedia.org/wiki/Greedy_algorithm) are the most desirable in multivariate feature selection scenarios because the wrapper methods are usually computationally very expensive and greedy algos don't necessarily provide the optimal solution, which is a good thing because it makes them less prone to overfitting. Forward Selection, Backward Elimination, Recursive Feature Elimination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.base import clone\n",
    "import itertools\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy.stats import f as f_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "datasource = \"datasets/winequality-red.csv\"\n",
    "print(os.path.exists(datasource))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.053</td>\n",
       "      <td>24.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.99402</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11.3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.086</td>\n",
       "      <td>20.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.99558</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.54</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.062</td>\n",
       "      <td>31.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99728</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.65</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.4</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.081</td>\n",
       "      <td>32.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.99640</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.72</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.071</td>\n",
       "      <td>15.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.66</td>\n",
       "      <td>10.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            6.2              0.56         0.09             1.7      0.053   \n",
       "1            7.0              0.36         0.21             2.3      0.086   \n",
       "2            7.3              0.48         0.32             2.1      0.062   \n",
       "3            8.4              0.36         0.32             2.2      0.081   \n",
       "4            7.1              0.53         0.07             1.7      0.071   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 24.0                  32.0  0.99402  3.54       0.60   \n",
       "1                 20.0                  65.0  0.99558  3.40       0.54   \n",
       "2                 31.0                  54.0  0.99728  3.30       0.65   \n",
       "3                 32.0                  79.0  0.99640  3.30       0.72   \n",
       "4                 15.0                  24.0  0.99510  3.29       0.66   \n",
       "\n",
       "   alcohol  quality  \n",
       "0     11.3        5  \n",
       "1     10.1        6  \n",
       "2     10.0        7  \n",
       "3     11.0        6  \n",
       "4     10.8        6  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(datasource).sample(frac = 1).reset_index(drop = True)\n",
    "del df[\"Unnamed: 0\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(df.iloc[:, :-1])\n",
    "y = np.array(df[\"quality\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection solution space\n",
    "\n",
    "From algorithm analysus' point of view, a solution for feature selection problems can be represented as a boolean vector, each component indicating whether the corresponding feature has been selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True False False  True  True False False False  True]\n"
     ]
    }
   ],
   "source": [
    "selected = np.array([False, True, True, False, False, True, True, False, False, False, True])\n",
    "print(selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn calls the corresponding indices to feature columns selected \"support\", which can be obtained using np.flatnonzero(). \n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.flatnonzero.html\n",
    "\n",
    "Return indices that are non-zero in the flattened version of a.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  5  6 10]\n"
     ]
    }
   ],
   "source": [
    "support = np.flatnonzero(selected)\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, a naive approach that exhaustively search all subsets of features would have to verify 2^p solutions. This is very inefficient. However, we will run an exhaustive search for all solutions that provide 5 features to establish a baseline. This limits our time complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_combinations(estimator, X, y, k = 5):\n",
    "    # fit and score model based on some subset of features\n",
    "    score = lambda X_features: clone(estimator).fit(X_features, y).score(X_features, y)\n",
    "    \n",
    "    # enumerate all combinations of 5 features\n",
    "    for subset in itertools.combinations(range(X.shape[1]), 5):\n",
    "        yield score(X[:, subset]), subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = search_combinations(LinearRegression(), X, y)\n",
    "\n",
    "# feed it a model, X, and y. \n",
    "# it'll iterate through all possible variations (up to 5)\n",
    "# and fit/score on those variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.35149423850184047, (1, 4, 6, 9, 10)),\n",
       " (0.34831403567393349, (1, 4, 8, 9, 10)),\n",
       " (0.34695788821029638, (1, 6, 8, 9, 10)),\n",
       " (0.34667490118495603, (0, 1, 4, 9, 10)),\n",
       " (0.34580097696162626, (0, 1, 6, 9, 10))]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(scores, reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Selection\n",
    "\n",
    "Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_select(estimator, X, y, k = 2):\n",
    "    # this array holds indicators of whether each feature is currently selected\n",
    "    selected = np.zeros(X.shape[1]).astype(bool)\n",
    "    \n",
    "    # fit and score the model based on some subset of features\n",
    "    score = lambda X_features: clone(estimator).fit(X_features, y).score(X_features, y)\n",
    "    \n",
    "    # find indices to selected columns\n",
    "    selected_indices = lambda: list(np.flatnonzero(selected))\n",
    "    \n",
    "    # repeated til k features are selected\n",
    "    while np.sum(selected) < k:\n",
    "        # indices to unselected column\n",
    "        rest_indices = list(np.flatnonzero(~selected))\n",
    "        \n",
    "        # compute model scores with an additional feature\n",
    "        scores = [score(X[:, selected_indices() + [i]]) for i in rest_indices]\n",
    "        \n",
    "        print(\"\\n * accuracy if adding column: \\n    \", {i: int(s * 100) for i, s in zip(rest_indices, scores)})\n",
    "        \n",
    "        # find index within \"rest_indices\" that points to the most predictive feature not yet selected\n",
    "        idx_to_add = rest_indices[np.argmax(scores)]\n",
    "        print(\"add column\", idx_to_add)\n",
    "        \n",
    "        # select this new feature\n",
    "        selected[idx_to_add] = True\n",
    "        print(\"================================\")\n",
    "        \n",
    "    return selected_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * accuracy if adding column: \n",
      "     {0: 1, 1: 15, 2: 5, 3: 0, 4: 1, 5: 0, 6: 3, 7: 3, 8: 0, 9: 6, 10: 22}\n",
      "add column 10\n",
      "================================\n",
      "\n",
      " * accuracy if adding column: \n",
      "     {0: 25, 1: 31, 2: 25, 3: 22, 4: 22, 5: 22, 6: 23, 7: 23, 8: 25, 9: 26}\n",
      "add column 1\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "support = sorted(forward_select(LinearRegression(), X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backwards Elimination\n",
    "\n",
    "In backwards elimination, we basically just do the opposite. We start with ALL the features and remove the least significant feature at each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backwards_eliminate(estimator, X, y, k = 5):\n",
    "    # this array holds indicators of whether each feature is currently selected\n",
    "    selected = np.ones(X.shape[1]).astype(bool)\n",
    "    \n",
    "    # fit and score model based on some subset of features\n",
    "    score = lambda X_features: clone(estimator).fit(X_features, y).score(X_features, y)\n",
    "    \n",
    "    # find indices to selected columns\n",
    "    selected_indices = lambda: list(np.flatnonzero(selected))\n",
    "    \n",
    "    # repeat til k features are selected\n",
    "    while np.sum(selected) > k:\n",
    "        # compute model scores with one of the features removed\n",
    "        scores = [score(X[:, list(set(selected_indices()) - {i})]) for i in selected_indices()]\n",
    "        print(\"\\n accuracy if removing column: \\n\", {i: int(s*100) for i, s in zip(selected_indices(), scores)})\n",
    "        \n",
    "        # find index that points to the least predictive feature\n",
    "        idx_to_remove = selected_indices()[np.argmax(scores)]\n",
    "        print(\"remove column\", idx_to_remove)\n",
    "        \n",
    "        # remove this feature\n",
    "        selected[idx_to_remove] = False\n",
    "        print(\"================================\")\n",
    "        \n",
    "    return selected_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " accuracy if removing column: \n",
      " {0: 36, 1: 32, 2: 35, 3: 36, 4: 35, 5: 35, 6: 35, 7: 36, 8: 35, 9: 33, 10: 31}\n",
      "remove column 7\n",
      "================================\n",
      "\n",
      " accuracy if removing column: \n",
      " {0: 36, 1: 32, 2: 35, 3: 36, 4: 35, 5: 35, 6: 35, 8: 35, 9: 33, 10: 24}\n",
      "remove column 0\n",
      "================================\n",
      "\n",
      " accuracy if removing column: \n",
      " {1: 32, 2: 35, 3: 35, 4: 35, 5: 35, 6: 35, 8: 35, 9: 33, 10: 24}\n",
      "remove column 3\n",
      "================================\n",
      "\n",
      " accuracy if removing column: \n",
      " {1: 32, 2: 35, 4: 35, 5: 35, 6: 35, 8: 35, 9: 33, 10: 24}\n",
      "remove column 2\n",
      "================================\n",
      "\n",
      " accuracy if removing column: \n",
      " {1: 31, 4: 34, 5: 35, 6: 34, 8: 35, 9: 33, 10: 24}\n",
      "remove column 5\n",
      "================================\n",
      "\n",
      " accuracy if removing column: \n",
      " {1: 31, 4: 34, 6: 34, 8: 35, 9: 33, 10: 23}\n",
      "remove column 8\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "support = sorted(backwards_eliminate(LinearRegression(), X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "Recursive feature elimination is an even more greedy algo. It finds good performing feature subset with high efficiency. The importance of each feature is obtained either through a coef attribute or through a feature_importances attribute. So in order for recursive feature elimination to work, the model is required to provide either of these attributes. \n",
    "\n",
    "We typically start off using a low complexity model and use it as a benchmark for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFE(estimator=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       "  n_features_to_select=5, step=1, verbose=0)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "selector = RFE(model, 5)\n",
    "selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Features:\", selector.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: [1 4 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(\"Selected Features:\", np.flatnonzero(selector.support_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we transform the dataset to include only these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 5)\n"
     ]
    }
   ],
   "source": [
    "X_new = selector.transform(X)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Methods\n",
    "\n",
    "These methods rank feature predictiveness one by one, as opposed to considering a subset. They incorporate statistical methods to rank each feature instead of measuring accuracy of a model trained on selected features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Peason's X^2 test based feature selection\n",
    "The following constructs the approximate X^2 distribution and scores each feature vs the label in order to determine which feature is more relevant, one at a time...and then selects features according to the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2, k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=5, score_func=<function chi2 at 0x00000181D77B2158>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X^2 statistic: \n",
      " [  1.12606524e+01   1.55802891e+01   1.30256651e+01   4.12329474e+00\n",
      "   7.52425579e-01   1.61936036e+02   2.75555798e+03   2.30432045e-04\n",
      "   1.54654736e-01   4.55848775e+00   4.64298922e+01]\n"
     ]
    }
   ],
   "source": [
    "print(\"X^2 statistic: \\n\", selector.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected indices: \n",
      " [ 1  2  5  6 10]\n"
     ]
    }
   ],
   "source": [
    "print(\"Selected indices: \\n\", selector.get_support(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then call transform() to select those feature columns from the dataset and store them into a new variable called X_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599, 5)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_selected = selector.transform(X)\n",
    "X_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can slice and dice. This does the same thing as transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(X_selected, X[:, [1, 2, 5, 6, 10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.56,   0.09,  24.  ,  32.  ,  11.3 ],\n",
       "       [  0.36,   0.21,  20.  ,  65.  ,  10.1 ],\n",
       "       [  0.48,   0.32,  31.  ,  54.  ,  10.  ],\n",
       "       ..., \n",
       "       [  0.29,   0.36,  35.  ,  53.  ,  12.4 ],\n",
       "       [  0.31,   0.51,  14.  ,  28.  ,   9.8 ],\n",
       "       [  0.44,   0.2 ,  24.  ,  64.  ,  11.7 ]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, [1, 2, 5, 6, 10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at this procedure so we can better distinguish different feature selection methods. X^2 test has many applications. For feature selection, we utilize X^2 statistic to test for dependence of each feature towards determining label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Encode labels into orthogonal vector space\n",
    "This is also known as one-hot encoding. It's applicable to classification problems. Consider an example where you have defined 3 categories for possible outcomes: A/B/C. In order for machine learning algos to be able to handle this type of data, we have to convert them to numbers. One-hot encoding uses a vector (y1, y2, y3) where yi = [result falls into the ith category]. \n",
    "\n",
    "Therefore, one-hot encoding for A, B, and C categories becomes (1, 0, 0), (0, 1, 0), and (0, 0, 1) respectively. This has an advantage over plainly translating A, B, C into 1, 2, 3 (aka sparse encoding) in a way that orthogonal vectors do not impose assumptions of their order or magnitudes between categories like numbers would. \n",
    "\n",
    "For example, 3 > 1 is true. However, it doesn't mean to imply that C > A or C is superior to A in any way. However, this would affect the model's numerical stability. \n",
    "\n",
    "Therefore, one-hot encoding is a widely adopted technique for processing categories. Sparse encoding could be used when persisting a dataset in order to save storage space. One-hot encoding is how the categorical/nominal variables are encoded as independent predictors in the regression formula. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = np.array(LabelBinarizer().fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 6)\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(Y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Compute the contingency table of observed frequencies\n",
    "\"Observed\" is a (# classes) - by - (# features) matrix that contains the \"number of occurrences\" for each combination of feature and classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observed = np.dot(Y.T, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.36000000e+01,   8.84500000e+00,   1.71000000e+00,\n",
       "          2.63500000e+01,   1.22500000e+00,   1.10000000e+02,\n",
       "          2.49000000e+02,   9.97464000e+00,   3.39800000e+01,\n",
       "          5.70000000e+00,   9.95500000e+01],\n",
       "       [  4.12300000e+02,   3.67800000e+01,   9.23000000e+00,\n",
       "          1.42800000e+02,   4.80600000e+00,   6.50000000e+02,\n",
       "          1.92100000e+03,   5.28167500e+01,   1.79220000e+02,\n",
       "          3.16100000e+01,   5.44050000e+02],\n",
       "       [  5.56190000e+03,   3.92965000e+02,   1.65950000e+02,\n",
       "          1.72215000e+03,   6.31530000e+01,   1.15660000e+04,\n",
       "          3.84860000e+04,   6.79027570e+02,   2.25067000e+03,\n",
       "          4.22880000e+02,   6.74170000e+03],\n",
       "       [  5.32550000e+03,   3.17395000e+02,   1.74700000e+02,\n",
       "          1.58045000e+03,   5.42020000e+01,   1.00240000e+04,\n",
       "          2.60750000e+04,   6.35840410e+02,   2.11693000e+03,\n",
       "          4.30860000e+02,   6.78163333e+03],\n",
       "       [  1.76560000e+03,   8.03800000e+01,   7.46600000e+01,\n",
       "          5.41400000e+02,   1.52410000e+01,   2.79500000e+03,\n",
       "          6.96900000e+03,   1.98224750e+02,   6.54860000e+02,\n",
       "          1.47510000e+02,   2.28171667e+03],\n",
       "       [  1.54200000e+02,   7.62000000e+00,   7.04000000e+00,\n",
       "          4.64000000e+01,   1.23200000e+00,   2.39000000e+02,\n",
       "          6.02000000e+02,   1.79138200e+01,   5.88100000e+01,\n",
       "          1.38200000e+01,   2.17700000e+02]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Compute the expected frequencies using marginal frequencies\n",
    "\"Expected\" will have the same shape as \"Observed\" matrix but represents the expected frequencies in theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected = np.dot(Y.mean(axis = 0).reshape(1, -1).T, # mean value for all classes (transposed) \n",
    "                 X.sum(axis = 0).reshape(1, -1)) # marginal frequencies for all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Compute the X^2 statistic between observed and expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.12606524e+01   1.55802891e+01   1.30256651e+01   4.12329474e+00\n",
      "   7.52425579e-01   1.61936036e+02   2.75555798e+03   2.30432045e-04\n",
      "   1.54654736e-01   4.55848775e+00   4.64298922e+01]\n"
     ]
    }
   ],
   "source": [
    "chi_squared = np.sum((observed - expected) ** 2 / expected, axis= 0)\n",
    "print(chi_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare against sklearn's chi2(), which returns 2 arrays. X^2 statistic and p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chi2_sklearn, pvalue_sklearn = chi2(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi Squared Statistic: \n",
      " [  1.12606524e+01   1.55802891e+01   1.30256651e+01   4.12329474e+00\n",
      "   7.52425579e-01   1.61936036e+02   2.75555798e+03   2.30432045e-04\n",
      "   1.54654736e-01   4.55848775e+00   4.64298922e+01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Chi Squared Statistic: \\n\", chi2_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P values: \n",
      " [  4.64500416e-02   8.15035154e-03   2.31394417e-02   5.31804675e-01\n",
      "   9.79968040e-01   3.82728810e-33   0.00000000e+00   1.00000000e+00\n",
      "   9.99526491e-01   4.72096321e-01   7.42403757e-09]\n"
     ]
    }
   ],
   "source": [
    "print(\"P values: \\n\", pvalue_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Rank features descendingly by X^2 statistic\n",
    "Optionally, then sort these indices so they remain in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 5, 6, 10]\n"
     ]
    }
   ],
   "source": [
    "support = sorted(np.argsort(-chi_squared)[0:5])\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Select these features and transform the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X[:, support]\n",
    "np.allclose(X_new, X_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the p value\n",
    "The p value is very useful because it could tell you quantitavely how probable each feature is relevant, which in turns helps you decide how many and which features are worthwhile to retain. Our null hypothesis is that a class label is independent of a feature. Since we used X^2 statistic for testing what the null hypotheis has claimed (independence between features and labels). We can choose a p value > 2.5% as the critical level at which the null hypothesis can not be rejected and reject the hypothesis otherwise. In other words, if the feature has more than 10% chance to be independent of the class, then it is not a good predictor. The alternative hypothesis is that the feature is a probable predictor of the class (aka the class is dependent on the feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr><td><strong>feature idx</strong></td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td></tr>\n",
    "<tr><td><strong>χ²</strong></td><td>11.3</td><td>15.6</td><td>13.0</td><td>4.12</td><td>0.752</td><td>162</td><td>2.76e+03</td><td>2.30e-04</td><td>0.155</td><td>4.56</td><td>46.4</td></tr>\n",
    "<tr><td><strong>p-value</strong></td><td>~5%</td><td>~0.5%</td><td>~1%</td><td>90%~95%</td><td>100%</td><td>0%</td><td>0%</td><td>100%</td><td>100%</td><td>90%~95%</td><td>0%</td></tr>\n",
    "<tr><td><strong>interpretation</strong></td><td>independent</td><td><strong>dependent</strong></td><td><strong>dependent</strong></td><td>independent</td><td>independent</td><td><strong>dependent</strong></td><td><strong>dependent</strong></td><td>independent</td><td>independent</td><td>independent</td><td><strong>dependent</strong></td></tr>\n",
    "</table>\n",
    "\n",
    "So we should select feature 1, 2, 5, 6 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function chi2 in module sklearn.feature_selection.univariate_selection:\n",
      "\n",
      "chi2(X, y)\n",
      "    Compute chi-squared stats between each non-negative feature and class.\n",
      "    \n",
      "    This score can be used to select the n_features features with the\n",
      "    highest values for the test chi-squared statistic from X, which must\n",
      "    contain only non-negative features such as booleans or frequencies\n",
      "    (e.g., term counts in document classification), relative to the classes.\n",
      "    \n",
      "    Recall that the chi-square test measures dependence between stochastic\n",
      "    variables, so using this function \"weeds out\" the features that are the\n",
      "    most likely to be independent of class and therefore irrelevant for\n",
      "    classification.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : {array-like, sparse matrix}, shape = (n_samples, n_features_in)\n",
      "        Sample vectors.\n",
      "    \n",
      "    y : array-like, shape = (n_samples,)\n",
      "        Target vector (class labels).\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    chi2 : array, shape = (n_features,)\n",
      "        chi2 statistics of each feature.\n",
      "    pval : array, shape = (n_features,)\n",
      "        p-values of each feature.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Complexity of this algorithm is O(n_classes * n_features).\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    f_classif: ANOVA F-value between label/feature for classification tasks.\n",
      "    f_regression: F-value between label/feature for regression tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(chi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a X^2 table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chi2_table(degree_of_freedoms):\n",
    "    from scipy.stats import chi2 as chi2_distribution\n",
    "    \n",
    "    pvalue = np.array([0.995, 0.99, 0.975, 0.95, 0.90, 0.10, 0.05, 0.025, 0.01, 0.005])\n",
    "    return pd.DataFrame(chi2_distribution.isf(pvalue, \n",
    "                                              np.expand_dims(degree_of_freedoms, 1)), \n",
    "                                              columns = pvalue, index = degree_of_freedoms)\n",
    "\n",
    "    # isf(p) = inverse(1 - cdf)(p) which takes p value returns chi square value\n",
    "    # where cdf is short for cumulative distribution function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.995</th>\n",
       "      <th>0.99</th>\n",
       "      <th>0.975</th>\n",
       "      <th>0.95</th>\n",
       "      <th>0.9</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.05</th>\n",
       "      <th>0.025</th>\n",
       "      <th>0.01</th>\n",
       "      <th>0.005</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.411742</td>\n",
       "      <td>0.554298</td>\n",
       "      <td>0.831212</td>\n",
       "      <td>1.145476</td>\n",
       "      <td>1.610308</td>\n",
       "      <td>9.236357</td>\n",
       "      <td>11.070498</td>\n",
       "      <td>12.832502</td>\n",
       "      <td>15.086272</td>\n",
       "      <td>16.749602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.675727</td>\n",
       "      <td>0.872090</td>\n",
       "      <td>1.237344</td>\n",
       "      <td>1.635383</td>\n",
       "      <td>2.204131</td>\n",
       "      <td>10.644641</td>\n",
       "      <td>12.591587</td>\n",
       "      <td>14.449375</td>\n",
       "      <td>16.811894</td>\n",
       "      <td>18.547584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.989256</td>\n",
       "      <td>1.239042</td>\n",
       "      <td>1.689869</td>\n",
       "      <td>2.167350</td>\n",
       "      <td>2.833107</td>\n",
       "      <td>12.017037</td>\n",
       "      <td>14.067140</td>\n",
       "      <td>16.012764</td>\n",
       "      <td>18.475307</td>\n",
       "      <td>20.277740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.344413</td>\n",
       "      <td>1.646497</td>\n",
       "      <td>2.179731</td>\n",
       "      <td>2.732637</td>\n",
       "      <td>3.489539</td>\n",
       "      <td>13.361566</td>\n",
       "      <td>15.507313</td>\n",
       "      <td>17.534546</td>\n",
       "      <td>20.090235</td>\n",
       "      <td>21.954955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.734933</td>\n",
       "      <td>2.087901</td>\n",
       "      <td>2.700389</td>\n",
       "      <td>3.325113</td>\n",
       "      <td>4.168159</td>\n",
       "      <td>14.683657</td>\n",
       "      <td>16.918978</td>\n",
       "      <td>19.022768</td>\n",
       "      <td>21.665994</td>\n",
       "      <td>23.589351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.155856</td>\n",
       "      <td>2.558212</td>\n",
       "      <td>3.246973</td>\n",
       "      <td>3.940299</td>\n",
       "      <td>4.865182</td>\n",
       "      <td>15.987179</td>\n",
       "      <td>18.307038</td>\n",
       "      <td>20.483177</td>\n",
       "      <td>23.209251</td>\n",
       "      <td>25.188180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.603222</td>\n",
       "      <td>3.053484</td>\n",
       "      <td>3.815748</td>\n",
       "      <td>4.574813</td>\n",
       "      <td>5.577785</td>\n",
       "      <td>17.275009</td>\n",
       "      <td>19.675138</td>\n",
       "      <td>21.920049</td>\n",
       "      <td>24.724970</td>\n",
       "      <td>26.756849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0.995     0.990     0.975     0.950     0.900      0.100      0.050  \\\n",
       "5   0.411742  0.554298  0.831212  1.145476  1.610308   9.236357  11.070498   \n",
       "6   0.675727  0.872090  1.237344  1.635383  2.204131  10.644641  12.591587   \n",
       "7   0.989256  1.239042  1.689869  2.167350  2.833107  12.017037  14.067140   \n",
       "8   1.344413  1.646497  2.179731  2.732637  3.489539  13.361566  15.507313   \n",
       "9   1.734933  2.087901  2.700389  3.325113  4.168159  14.683657  16.918978   \n",
       "10  2.155856  2.558212  3.246973  3.940299  4.865182  15.987179  18.307038   \n",
       "11  2.603222  3.053484  3.815748  4.574813  5.577785  17.275009  19.675138   \n",
       "\n",
       "        0.025      0.010      0.005  \n",
       "5   12.832502  15.086272  16.749602  \n",
       "6   14.449375  16.811894  18.547584  \n",
       "7   16.012764  18.475307  20.277740  \n",
       "8   17.534546  20.090235  21.954955  \n",
       "9   19.022768  21.665994  23.589351  \n",
       "10  20.483177  23.209251  25.188180  \n",
       "11  21.920049  24.724970  26.756849  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi2_table(range(5, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA F-value based features selection\n",
    "\n",
    "The following calculates Pearson's correlation of each feature vs the label in order to determine which feature is more relevant, one at a time, then selects features according to the ANOVA F-Value derived from Pearson's correlation. \n",
    "\n",
    "https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.pearsonr.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=5, score_func=<function f_regression at 0x00000181D77B21E0>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SelectKBest(f_regression, k = 5)\n",
    "selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: \n",
      " [  2.49600375e+01   2.87444450e+02   8.62577262e+01   3.01183699e-01\n",
      "   2.69856084e+01   4.10850227e+00   5.66578176e+01   5.04052231e+01\n",
      "   5.34046221e+00   1.07740433e+02   4.68267011e+02]\n"
     ]
    }
   ],
   "source": [
    "print(\"Score: \\n\", selector.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Indices: \n",
      " [ 1  2  6  9 10]\n"
     ]
    }
   ],
   "source": [
    "print(\"Selected Indices: \\n\", selector.get_support(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.56   0.09  32.     0.6   11.3 ]\n",
      " [  0.36   0.21  65.     0.54  10.1 ]\n",
      " [  0.48   0.32  54.     0.65  10.  ]\n",
      " ..., \n",
      " [  0.29   0.36  53.     1.01  12.4 ]\n",
      " [  0.31   0.51  28.     0.93   9.8 ]\n",
      " [  0.44   0.2   64.     0.57  11.7 ]]\n"
     ]
    }
   ],
   "source": [
    "X_new = selector.transform(X)\n",
    "print(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F test can be used as hypothesis testing for a ratio of two X^2 distributions. The F-test for correlation coefficients is derived from testing a ratio between regression sum square (SSR) and squared sum error (SSE). \n",
    "\n",
    "$$ SSR = \\sum _{i=1}^n {\\left(\\hat {y_i} - \\bar y \\right)^2}$$\n",
    "\n",
    "$$ SSE = \\sum _{i=1}^n {\\left(\\hat {y_i} - y_i \\right)^2}$$\n",
    "\n",
    "$$ F  = \\frac{SSR/(v-1) }{SSE/(n-2)} $$\n",
    "\n",
    "With correlation coefficients, this is \n",
    "\n",
    "$$ F = \\frac{r^2/1 }{(1-r^2)/(n-2)} $$\n",
    "\n",
    "with degree of freedom 1 and (n-2) respectively.\n",
    "\n",
    "Its null hypothesis claims r = 0, i.e. the independence of each feature and label.\n",
    "We could theoretically make hypothesis testing using an F-distribution table.\n",
    "\n",
    "Here's how to generate F table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_table(alpha, v1, v2):\n",
    "    from scipy.stats import f as f_distribution\n",
    "    v1 = np.array(list(v1))\n",
    "    v2 = np.array(list(v2))\n",
    "    return pd.DataFrame(f_distribution.isf(alpha, v1[np.newaxis, ...], v2[..., np.newaxis]),\n",
    "                           columns = v1, index = v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.863458</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>53.593245</td>\n",
       "      <td>55.832961</td>\n",
       "      <td>57.240077</td>\n",
       "      <td>58.204416</td>\n",
       "      <td>58.905953</td>\n",
       "      <td>59.438981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.526316</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.161790</td>\n",
       "      <td>9.243416</td>\n",
       "      <td>9.292626</td>\n",
       "      <td>9.325530</td>\n",
       "      <td>9.349081</td>\n",
       "      <td>9.366770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.538319</td>\n",
       "      <td>5.462383</td>\n",
       "      <td>5.390773</td>\n",
       "      <td>5.342644</td>\n",
       "      <td>5.309157</td>\n",
       "      <td>5.284732</td>\n",
       "      <td>5.266195</td>\n",
       "      <td>5.251671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.544771</td>\n",
       "      <td>4.324555</td>\n",
       "      <td>4.190860</td>\n",
       "      <td>4.107250</td>\n",
       "      <td>4.050579</td>\n",
       "      <td>4.009749</td>\n",
       "      <td>3.978966</td>\n",
       "      <td>3.954940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.060420</td>\n",
       "      <td>3.779716</td>\n",
       "      <td>3.619477</td>\n",
       "      <td>3.520196</td>\n",
       "      <td>3.452982</td>\n",
       "      <td>3.404507</td>\n",
       "      <td>3.367899</td>\n",
       "      <td>3.339276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.775950</td>\n",
       "      <td>3.463304</td>\n",
       "      <td>3.288762</td>\n",
       "      <td>3.180763</td>\n",
       "      <td>3.107512</td>\n",
       "      <td>3.054551</td>\n",
       "      <td>3.014457</td>\n",
       "      <td>2.983036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.589428</td>\n",
       "      <td>3.257442</td>\n",
       "      <td>3.074072</td>\n",
       "      <td>2.960534</td>\n",
       "      <td>2.883344</td>\n",
       "      <td>2.827392</td>\n",
       "      <td>2.784930</td>\n",
       "      <td>2.751580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.457919</td>\n",
       "      <td>3.113118</td>\n",
       "      <td>2.923796</td>\n",
       "      <td>2.806426</td>\n",
       "      <td>2.726447</td>\n",
       "      <td>2.668335</td>\n",
       "      <td>2.624135</td>\n",
       "      <td>2.589349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.360303</td>\n",
       "      <td>3.006452</td>\n",
       "      <td>2.812863</td>\n",
       "      <td>2.692680</td>\n",
       "      <td>2.610613</td>\n",
       "      <td>2.550855</td>\n",
       "      <td>2.505313</td>\n",
       "      <td>2.469406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.285015</td>\n",
       "      <td>2.924466</td>\n",
       "      <td>2.727673</td>\n",
       "      <td>2.605336</td>\n",
       "      <td>2.521641</td>\n",
       "      <td>2.460582</td>\n",
       "      <td>2.413965</td>\n",
       "      <td>2.377150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.880695</td>\n",
       "      <td>2.488716</td>\n",
       "      <td>2.276071</td>\n",
       "      <td>2.142235</td>\n",
       "      <td>2.049246</td>\n",
       "      <td>1.980333</td>\n",
       "      <td>1.926916</td>\n",
       "      <td>1.884121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2.808658</td>\n",
       "      <td>2.411955</td>\n",
       "      <td>2.196730</td>\n",
       "      <td>2.060816</td>\n",
       "      <td>1.965999</td>\n",
       "      <td>1.895431</td>\n",
       "      <td>1.840496</td>\n",
       "      <td>1.796300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2.778604</td>\n",
       "      <td>2.380015</td>\n",
       "      <td>2.163735</td>\n",
       "      <td>2.026947</td>\n",
       "      <td>1.931343</td>\n",
       "      <td>1.860049</td>\n",
       "      <td>1.804438</td>\n",
       "      <td>1.759607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2.762115</td>\n",
       "      <td>2.362513</td>\n",
       "      <td>2.145660</td>\n",
       "      <td>2.008390</td>\n",
       "      <td>1.912348</td>\n",
       "      <td>1.840645</td>\n",
       "      <td>1.784650</td>\n",
       "      <td>1.739457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2.751698</td>\n",
       "      <td>2.351464</td>\n",
       "      <td>2.134251</td>\n",
       "      <td>1.996676</td>\n",
       "      <td>1.900354</td>\n",
       "      <td>1.828389</td>\n",
       "      <td>1.772147</td>\n",
       "      <td>1.726719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2.744520</td>\n",
       "      <td>2.343855</td>\n",
       "      <td>2.126395</td>\n",
       "      <td>1.988609</td>\n",
       "      <td>1.892093</td>\n",
       "      <td>1.819946</td>\n",
       "      <td>1.763531</td>\n",
       "      <td>1.717939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2.739275</td>\n",
       "      <td>2.338296</td>\n",
       "      <td>2.120656</td>\n",
       "      <td>1.982716</td>\n",
       "      <td>1.886057</td>\n",
       "      <td>1.813776</td>\n",
       "      <td>1.757233</td>\n",
       "      <td>1.711520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2.735274</td>\n",
       "      <td>2.334056</td>\n",
       "      <td>2.116279</td>\n",
       "      <td>1.978222</td>\n",
       "      <td>1.881454</td>\n",
       "      <td>1.809070</td>\n",
       "      <td>1.752429</td>\n",
       "      <td>1.706623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2.732121</td>\n",
       "      <td>2.330717</td>\n",
       "      <td>2.112832</td>\n",
       "      <td>1.974681</td>\n",
       "      <td>1.877827</td>\n",
       "      <td>1.805362</td>\n",
       "      <td>1.748644</td>\n",
       "      <td>1.702763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>2.729573</td>\n",
       "      <td>2.328018</td>\n",
       "      <td>2.110046</td>\n",
       "      <td>1.971820</td>\n",
       "      <td>1.874896</td>\n",
       "      <td>1.802365</td>\n",
       "      <td>1.745584</td>\n",
       "      <td>1.699643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>2.727471</td>\n",
       "      <td>2.325791</td>\n",
       "      <td>2.107748</td>\n",
       "      <td>1.969460</td>\n",
       "      <td>1.872478</td>\n",
       "      <td>1.799892</td>\n",
       "      <td>1.743059</td>\n",
       "      <td>1.697068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>2.725707</td>\n",
       "      <td>2.323924</td>\n",
       "      <td>2.105819</td>\n",
       "      <td>1.967480</td>\n",
       "      <td>1.870450</td>\n",
       "      <td>1.797818</td>\n",
       "      <td>1.740941</td>\n",
       "      <td>1.694908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             1          2          3          4          5          6  \\\n",
       "1    39.863458  49.500000  53.593245  55.832961  57.240077  58.204416   \n",
       "2     8.526316   9.000000   9.161790   9.243416   9.292626   9.325530   \n",
       "3     5.538319   5.462383   5.390773   5.342644   5.309157   5.284732   \n",
       "4     4.544771   4.324555   4.190860   4.107250   4.050579   4.009749   \n",
       "5     4.060420   3.779716   3.619477   3.520196   3.452982   3.404507   \n",
       "6     3.775950   3.463304   3.288762   3.180763   3.107512   3.054551   \n",
       "7     3.589428   3.257442   3.074072   2.960534   2.883344   2.827392   \n",
       "8     3.457919   3.113118   2.923796   2.806426   2.726447   2.668335   \n",
       "9     3.360303   3.006452   2.812863   2.692680   2.610613   2.550855   \n",
       "10    3.285015   2.924466   2.727673   2.605336   2.521641   2.460582   \n",
       "30    2.880695   2.488716   2.276071   2.142235   2.049246   1.980333   \n",
       "50    2.808658   2.411955   2.196730   2.060816   1.965999   1.895431   \n",
       "70    2.778604   2.380015   2.163735   2.026947   1.931343   1.860049   \n",
       "90    2.762115   2.362513   2.145660   2.008390   1.912348   1.840645   \n",
       "110   2.751698   2.351464   2.134251   1.996676   1.900354   1.828389   \n",
       "130   2.744520   2.343855   2.126395   1.988609   1.892093   1.819946   \n",
       "150   2.739275   2.338296   2.120656   1.982716   1.886057   1.813776   \n",
       "170   2.735274   2.334056   2.116279   1.978222   1.881454   1.809070   \n",
       "190   2.732121   2.330717   2.112832   1.974681   1.877827   1.805362   \n",
       "210   2.729573   2.328018   2.110046   1.971820   1.874896   1.802365   \n",
       "230   2.727471   2.325791   2.107748   1.969460   1.872478   1.799892   \n",
       "250   2.725707   2.323924   2.105819   1.967480   1.870450   1.797818   \n",
       "\n",
       "             7          8  \n",
       "1    58.905953  59.438981  \n",
       "2     9.349081   9.366770  \n",
       "3     5.266195   5.251671  \n",
       "4     3.978966   3.954940  \n",
       "5     3.367899   3.339276  \n",
       "6     3.014457   2.983036  \n",
       "7     2.784930   2.751580  \n",
       "8     2.624135   2.589349  \n",
       "9     2.505313   2.469406  \n",
       "10    2.413965   2.377150  \n",
       "30    1.926916   1.884121  \n",
       "50    1.840496   1.796300  \n",
       "70    1.804438   1.759607  \n",
       "90    1.784650   1.739457  \n",
       "110   1.772147   1.726719  \n",
       "130   1.763531   1.717939  \n",
       "150   1.757233   1.711520  \n",
       "170   1.752429   1.706623  \n",
       "190   1.748644   1.702763  \n",
       "210   1.745584   1.699643  \n",
       "230   1.743059   1.697068  \n",
       "250   1.740941   1.694908  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_table(0.1, range(1, 9), itertools.chain(range(1,10), range(10, 260, 20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X^2 and F-value based feature selection work better on classification and regression respectively. The general steps for feature selection with sklearn was:\n",
    "1) Choose a feature scoring method\n",
    "2) Initialize a feature selector\n",
    "3) Fit feature selector on the data\n",
    "\n",
    "Other feature selection methods provided by sklearn include:\n",
    "* Classification: chi2, f_classif, mutual_info_classif\n",
    "* Regression: f_regression, mutual_info_regression\n",
    "\n",
    "Wrapper methods are computationally expensive. Greedy algos don't necessary provide the optimal solution (which may be good because it makes them less prone to overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
