{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis\n",
    "\n",
    "**Reference: ** An interesting read for the introduction to [Natural Language Processing](https://blog.monkeylearn.com/the-definitive-guide-to-natural-language-processing/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Data\n",
    "\n",
    "Lets get started with our text analysis with the twenty newsgroup data set. The data can be downloaded using the inbuilt methods of scikit learn library. We will explore the data a bit before delving into text analysis.\n",
    "\n",
    "One primary use of the early Internet was to share information among interested groups via newsgroups. Users could subscribe to these groups to send and receive postings of interest. This dataset has postings to twenty newsgroups, thus the newsgroup is the classification target and the text in the posting is used to make the features. The postings are similar to emails, thus each posting will have a header, the article body, which might quote all or part of a previous message, and possibly a footer. The header, quoted text, and the footer can be removed by scikit learn by including the remove attribute, and indicating whether these sections should be removed. This attribute can take one or all of the values: header, footer, and quotes. For example, the following attribute would be used to remove both headers and footers.\n",
    "\n",
    "    `remove =('headers', 'footers')`\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn.datasets.fetch_20newsgroups() is a data fetching / caching function that downloads the data archive from the original 20 newsgroups website, extracts the archive contents in the ~/scikit_learn_data/20news_home folder and calls the sklearn.datasets.load_files. There are train and test version of this data that you can load by supplying subset='train'/'test' option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "text_data = fetch_20newsgroups(data_home='/dsa/data/DSA-8630/newsgroups/')\n",
    "\n",
    "# To learn more about these data, use scikit learn documentation, or enter help(text_data) in an IPython code cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['description', 'target_names', 'DESCR', 'filenames', 'target', 'data'])\n"
     ]
    }
   ],
   "source": [
    "# The data can be accessed via Dictionary keys\n",
    "print(text_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 4 4 ..., 3 1 8]\n"
     ]
    }
   ],
   "source": [
    "print(text_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ '/dsa/data/DSA-8630/newsgroups/20news_home/20news-bydate-train/rec.autos/102994'\n",
      " '/dsa/data/DSA-8630/newsgroups/20news_home/20news-bydate-train/comp.sys.mac.hardware/51861'\n",
      " '/dsa/data/DSA-8630/newsgroups/20news_home/20news-bydate-train/comp.sys.mac.hardware/51879'\n",
      " ...,\n",
      " '/dsa/data/DSA-8630/newsgroups/20news_home/20news-bydate-train/comp.sys.ibm.pc.hardware/60695'\n",
      " '/dsa/data/DSA-8630/newsgroups/20news_home/20news-bydate-train/comp.graphics/38319'\n",
      " '/dsa/data/DSA-8630/newsgroups/20news_home/20news-bydate-train/rec.motorcycles/104440']\n"
     ]
    }
   ],
   "source": [
    "print(text_data['filenames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(text_data['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "The real data lies in the filenames and target attributes. The target attribute is the integer index of the category. Lets print the number of records in the data at hand. Also print first target names for first 10 rows.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314,)\n",
      "(11314,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(text_data.filenames.shape)\n",
    "\n",
    "print(text_data.target.shape)\n",
    "\n",
    "text_data.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 = alt.atheism\n",
      "Class 1 = comp.graphics\n",
      "Class 2 = comp.os.ms-windows.misc\n",
      "Class 3 = comp.sys.ibm.pc.hardware\n",
      "Class 4 = comp.sys.mac.hardware\n",
      "Class 5 = comp.windows.x\n",
      "Class 6 = misc.forsale\n",
      "Class 7 = rec.autos\n",
      "Class 8 = rec.motorcycles\n",
      "Class 9 = rec.sport.baseball\n",
      "Class 10 = rec.sport.hockey\n",
      "Class 11 = sci.crypt\n",
      "Class 12 = sci.electronics\n",
      "Class 13 = sci.med\n",
      "Class 14 = sci.space\n",
      "Class 15 = soc.religion.christian\n",
      "Class 16 = talk.politics.guns\n",
      "Class 17 = talk.politics.mideast\n",
      "Class 18 = talk.politics.misc\n",
      "Class 19 = talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "# Display target names, i.e., the names of the twenty news groups\n",
    "\n",
    "for index, label in enumerate(text_data['target_names']):\n",
    "    print('Class {0} = {1}'.format(index, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "In above print statement {0} represents first argument in print statement. Here it is index variable, {1} refers to second argument which is label here. \n",
    "\n",
    "**Reference: ** [enumerate()](https://docs.python.org/2.3/whatsnew/section-enumerate.html)\n",
    "\n",
    "Lets display a single message and see what all data is stored. If you look at the following line of code in below code cell,\n",
    "\n",
    "    text_data['target_names'][text_data['target'][messageID]]\n",
    "\n",
    "target is similar to target_names except that it is numeric coding of names. The second part of the code i'e, **text_data['target'][messageID]** will give the target number of 1st message. The first part i'e **text_data['target_names'][second part of code]** then uses this target number to print the target_name\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "[7 4 4 ..., 3 1 8]\n",
      "2\n",
      "=======\n",
      "Target Newsgroup: comp.os.ms-windows.misc\n",
      "----------------------------------------------------------------------\n",
      "From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\n",
      "Subject: Diamond SS24X, Win 3.1, Mouse cursor\n",
      "Organization: National Library of Medicine\n",
      "Lines: 10\n",
      "\n",
      "\n",
      "Anybody seen mouse cursor distortion running the Diamond 1024x768x256 driver?\n",
      "Sorry, don't know the version of the driver (no indication in the menus) but it's a recently\n",
      "delivered Gateway system.  Am going to try the latest drivers from Diamond BBS but wondered\n",
      "if anyone else had seen this.\n",
      "\n",
      "post or email\n",
      "\n",
      "--Don Lindbergh\n",
      "dabl2@lhc.nlm.nih.gov\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messageID = 1000\n",
    "\n",
    "print(text_data['target_names'])\n",
    "print(text_data['target'])\n",
    "print(text_data['target'][messageID])\n",
    "print(\"=======\")\n",
    "\n",
    "print('Target Newsgroup: {0}'.format(text_data['target_names'][text_data['target'][messageID]]))\n",
    "print(70*'-')\n",
    "\n",
    "message = text_data['data'][messageID]\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can analyze text messages by using basic Python commands. For example, we can find how many times a word appears in a text by using Python string functions. One important item to consider, however, is that, by default, Python will search for sequences of characters in a text message. Thus, if a word is also part of larger words, we will over-count the occurrences, as demonstrated in the following code cell. \n",
    "\n",
    "Observe the output of two print statements below. format(token) is inserting a space before 'to' so that it can be used to search for the word 'to'. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plain word: to\n",
      "formatted word:  to \n"
     ]
    }
   ],
   "source": [
    "token = 'to'\n",
    "\n",
    "# Create isolated words. words that are not part of any larger word. For example, to in 'distortion' is not counted as a word.\n",
    "# format(token) will result in creating a token which can be used as stanalone word for search.\n",
    "i_token = ' {0} '.format(token)\n",
    "\n",
    "print('plain word:',token)\n",
    "print('formatted word:',i_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The word 'to' has appeared twice i'e in 'distortion' and as 'to'. So when you search for number of times 'to' appeared in the text it will return 2. To avoid this, the formatted text resulted in creating a token that matched only the stanalone word 'to' resulting in count 1\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expression over count: 2\n",
      "Isolated Token Count: 1\n"
     ]
    }
   ],
   "source": [
    "print('Expression over count: {0}'.format(message.count(token)))\n",
    "print('Isolated Token Count: {0}'.format(message.count(i_token)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Above limitation where expression is over counted can be over come by explicitly splitting a text into tokens. By default, in Python this is done at whitespace, but this can be changed using regular expressions which we will come across later in this notebook. \n",
    "\n",
    "\n",
    "**Reference: ** [format()](https://docs.python.org/2/tutorial/inputoutput.html) can be used to format Input/Output operations in print statements. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From:', 'dabl2@nlm.nih.gov', '(Don', 'A.B.', 'Lindbergh)', 'Subject:', 'Diamond', 'SS24X,', 'Win', '3.1,', 'Mouse', 'cursor', 'Organization:', 'National', 'Library', 'of', 'Medicine', 'Lines:', '10', 'Anybody', 'seen', 'mouse', 'cursor', 'distortion', 'running', 'the', 'Diamond', '1024x768x256', 'driver?', 'Sorry,', \"don't\", 'know', 'the', 'version', 'of', 'the', 'driver', '(no', 'indication', 'in', 'the', 'menus)', 'but', \"it's\", 'a', 'recently', 'delivered', 'Gateway', 'system.', 'Am', 'going', 'to', 'try', 'the', 'latest', 'drivers', 'from', 'Diamond', 'BBS', 'but', 'wondered', 'if', 'anyone', 'else', 'had', 'seen', 'this.', 'post', 'or', 'email', '--Don', 'Lindbergh', 'dabl2@lhc.nlm.nih.gov']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accumulate counts of tokens, using string functionality\n",
    "import collections as col\n",
    "\n",
    "# Used to print sequences in a nice manner\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80,compact=True)\n",
    "\n",
    "# Tokenize the message and create a counter for frequency of each word in message.\n",
    "# Browse for split() python or go to this link http://www.pythonforbeginners.com/dictionary/python-split to see what split() does\n",
    "words = message.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 5, 'Diamond': 3, 'but': 2, 'seen': 2, 'cursor': 2, 'of': 2, 'going': 1, 'Lindbergh': 1, 'anyone': 1, 'Lindbergh)': 1, 'system.': 1, 'drivers': 1, 'From:': 1, 'Subject:': 1, '1024x768x256': 1, 'driver': 1, 'had': 1, 'email': 1, 'Anybody': 1, 'Sorry,': 1, '(no': 1, 'a': 1, 'indication': 1, '10': 1, 'if': 1, 'driver?': 1, 'to': 1, 'Am': 1, 'delivered': 1, 'menus)': 1, 'Library': 1, 'A.B.': 1, 'dabl2@nlm.nih.gov': 1, 'mouse': 1, 'Lines:': 1, 'wondered': 1, 'or': 1, \"don't\": 1, 'National': 1, 'Mouse': 1, 'version': 1, 'recently': 1, 'Organization:': 1, 'distortion': 1, 'BBS': 1, '3.1,': 1, 'from': 1, 'Gateway': 1, 'post': 1, 'know': 1, 'SS24X,': 1, 'else': 1, 'latest': 1, \"it's\": 1, 'Medicine': 1, 'dabl2@lhc.nlm.nih.gov': 1, 'Win': 1, '--Don': 1, 'this.': 1, '(Don': 1, 'try': 1, 'in': 1, 'running': 1})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = col.Counter(words)\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens = 63\n",
      "------------------------------\n",
      "Top 40 tokens by frequency:\n",
      "------------------------------\n",
      "[ ('the', 5), ('Diamond', 3), ('but', 2), ('seen', 2), ('cursor', 2), ('of', 2),\n",
      "  ('going', 1), ('Lindbergh', 1), ('anyone', 1), ('Lindbergh)', 1),\n",
      "  ('system.', 1), ('drivers', 1), ('From:', 1), ('Subject:', 1),\n",
      "  ('1024x768x256', 1), ('driver', 1), ('had', 1), ('email', 1), ('Anybody', 1),\n",
      "  ('Sorry,', 1), ('(no', 1), ('a', 1), ('indication', 1), ('10', 1), ('if', 1),\n",
      "  ('driver?', 1), ('to', 1), ('Am', 1), ('delivered', 1), ('menus)', 1),\n",
      "  ('Library', 1), ('A.B.', 1), ('dabl2@nlm.nih.gov', 1), ('mouse', 1),\n",
      "  ('Lines:', 1), ('wondered', 1), ('or', 1), (\"don't\", 1), ('National', 1),\n",
      "  ('Mouse', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Setting the limit to 40 for the number of tokens to display \n",
    "counts_to_display = 40\n",
    "\n",
    "# Display results. \n",
    "print('Total number of tokens = {0}'.format(len(word_count)))\n",
    "print(30*'-')\n",
    "print('Top {} tokens by frequency:'.format(counts_to_display))\n",
    "print(30*'-')\n",
    "pp.pprint(word_count.most_common(counts_to_display))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "**Reference: ** [counter](https://docs.python.org/2/library/collections.html#collections.Counter): A Counter is a subclass in collections class for counting hashable objects. It is an unordered collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. most_common() is a method in Counter class to that returns a list of the n most common elements and their counts from the most common to the least. In previous cell we used most_common() method to print the 40 most common words in the text accrording to their frequency.\n",
    "\n",
    "The previous cells tokenized a text document, but identical tokens with different case will be treated as distinct which is not an ideal behavior since it could undercount the occurrences of an otherwise important token. Convert text to lowercase to prevent this, by using the string lower method. The word mouse has appeared twice as [mouse, Mouse] before converting the text to lowercase. After making it to lowercase, the total number of tokens has decreased by 1, as well as the counts of specific tokens has chnaged(such as mouse count is increased from 1 to 2 while Mouse si no longer a token).\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens = 62\n",
      "------------------------------\n",
      "Top 40 tokens by frequency:\n",
      "------------------------------\n",
      "[ ('the', 5), ('diamond', 3), ('but', 2), ('seen', 2), ('mouse', 2),\n",
      "  ('cursor', 2), ('of', 2), ('going', 1), ('anybody', 1), ('anyone', 1),\n",
      "  ('system.', 1), ('drivers', 1), ('gateway', 1), ('a.b.', 1), ('subject:', 1),\n",
      "  ('or', 1), ('driver', 1), ('had', 1), ('email', 1), ('national', 1),\n",
      "  ('(no', 1), ('a', 1), ('indication', 1), ('10', 1), ('library', 1),\n",
      "  ('(don', 1), ('am', 1), ('driver?', 1), ('to', 1), ('--don', 1),\n",
      "  ('delivered', 1), ('menus)', 1), ('medicine', 1), ('lindbergh)', 1),\n",
      "  ('dabl2@nlm.nih.gov', 1), ('this.', 1), ('bbs', 1), (\"don't\", 1),\n",
      "  ('sorry,', 1), ('1024x768x256', 1)]\n"
     ]
    }
   ],
   "source": [
    "words = message.lower().split()\n",
    "word_count = col.Counter(words)\n",
    "\n",
    "# Setting the limit to 40 for the number of tokens to display \n",
    "counts_to_display = 40\n",
    "\n",
    "# Display results. \n",
    "print('Total number of tokens = {0}'.format(len(word_count)))\n",
    "print(30*'-')\n",
    "print('Top {} tokens by frequency:'.format(counts_to_display))\n",
    "print(30*'-')\n",
    "pp.pprint(word_count.most_common(counts_to_display))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Vector Space Model\n",
    "\n",
    "\n",
    "The traditional machine learning algorithms can only operate directly on numerical data. A text document can be analyzed by generating a numerical representation by counting the number of times a word occurs (as we did with the Counter collection previously. Another approach is to normalize the token counts by the total number of tokens, which creates a term (or token) frequency. In below code cell, the top terms and their frequency in the message is displayed.\n",
    "\n",
    "\n",
    "**Reference: ** \n",
    "\n",
    "- Wikipedia article on [Vector Space model](https://en.wikipedia.org/wiki/Vector_space_model)\n",
    "- Wikipedia article on [Document Term Matrix](https://en.wikipedia.org/wiki/Document-term_matrix)\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term        : Frequency\n",
      "--------------------\n",
      "'the         : 0.068'\n",
      "'diamond     : 0.041'\n",
      "'but         : 0.027'\n",
      "'seen        : 0.027'\n",
      "'mouse       : 0.027'\n",
      "'cursor      : 0.027'\n",
      "'of          : 0.027'\n",
      "'going       : 0.014'\n",
      "'anybody     : 0.014'\n",
      "'anyone      : 0.014'\n",
      "'system.     : 0.014'\n",
      "'drivers     : 0.014'\n",
      "'gateway     : 0.014'\n",
      "'a.b.        : 0.014'\n",
      "'subject:    : 0.014'\n",
      "'or          : 0.014'\n",
      "'driver      : 0.014'\n",
      "'had         : 0.014'\n",
      "'email       : 0.014'\n",
      "'national    : 0.014'\n",
      "'(no         : 0.014'\n",
      "'a           : 0.014'\n",
      "'indication  : 0.014'\n",
      "'10          : 0.014'\n",
      "'library     : 0.014'\n",
      "'(don        : 0.014'\n",
      "'am          : 0.014'\n",
      "'driver?     : 0.014'\n",
      "'to          : 0.014'\n",
      "'--don       : 0.014'\n",
      "'delivered   : 0.014'\n",
      "'menus)      : 0.014'\n",
      "'medicine    : 0.014'\n",
      "'lindbergh)  : 0.014'\n",
      "'dabl2@nlm.nih.gov: 0.014'\n",
      "'this.       : 0.014'\n",
      "'bbs         : 0.014'\n",
      "\"don't       : 0.014\"\n",
      "'sorry,      : 0.014'\n",
      "'1024x768x256: 0.014'\n"
     ]
    }
   ],
   "source": [
    "# In below print statememt, {0:12s} means, print argument 1 with 12 spaces allocated for it. \n",
    "print('{0:12s}: {1}'.format('Term', 'Frequency'))\n",
    "print(20*'-')\n",
    "\n",
    "total_word_count = sum(word_count.values())\n",
    "for count in word_count.most_common(counts_to_display):\n",
    "    pp.pprint('{0:12s}: {1:4.3f}'.format(count[0], count[1]/total_word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Bag of words\n",
    "\n",
    "Did it occur to your mind, how can you classify text data made up of words when machine learning algorithms work on numerical data. The only way is to build a numerical summary of the data that the algorithms can work on. An easy approach to implement this idea is to identify all possible words in the documents and to track the number of times each word occurs. In the context of this notebook, each post is a document. This produces a (very) sparse matrix of the documents, where the columns are the possible words (or tokens) and the rows are different documents (here posts).\n",
    "\n",
    "\n",
    "This concept, where one tokenizes documents to build these sparse matrices is more formally known as bag of words, because we effectively create the bag of words out of which are documents are constructed. In this model, each document is mapped into a vector, where the individual elements in the vector correspond to the number of times the words (associated with the particular column) appears in the document.\n",
    "\n",
    "For example, in the sentence, \"This is a great place to eat. I would recommend this place to my friends,\" the word this is seen twice, the word place is seen twice, the word great is seen once. There's one feature for each word in bag of words. The data has to be pre processed like every other data. It will dramatically improve the performance of the Bag of Words method. \n",
    "\n",
    "**Reference: ** Wikipedia article on [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) model\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "\n",
    "With scikit learn, the CountVectorizer can be used to break the documents into tokens (in this case words), which are used to construct bag of words for the posts. Given this tokenizer, we first need to construct the list of tokens, which we do with the fit method. Second, we need to transform our documents into this sparse matrix, which we do with the transform method. Since both steps use the same input data, there is a convenience method to perform both operations at the same time, called fit_transform.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define our vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer='word', lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\", 'From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\\nSubject: PB questions...\\nOrganization: Purdue University Engineering Computer Network\\nDistribution: usa\\nLines: 36\\n\\nwell folks, my mac plus finally gave up the ghost this weekend after\\nstarting life as a 512k way back in 1985.  sooo, i\\'m in the market for a\\nnew machine a bit sooner than i intended to be...\\n\\ni\\'m looking into picking up a powerbook 160 or maybe 180 and have a bunch\\nof questions that (hopefully) somebody can answer:\\n\\n* does anybody know any dirt on when the next round of powerbook\\nintroductions are expected?  i\\'d heard the 185c was supposed to make an\\nappearence \"this summer\" but haven\\'t heard anymore on it - and since i\\ndon\\'t have access to macleak, i was wondering if anybody out there had\\nmore info...\\n\\n* has anybody heard rumors about price drops to the powerbook line like the\\nones the duo\\'s just went through recently?\\n\\n* what\\'s the impression of the display on the 180?  i could probably swing\\na 180 if i got the 80Mb disk rather than the 120, but i don\\'t really have\\na feel for how much \"better\" the display is (yea, it looks great in the\\nstore, but is that all \"wow\" or is it really that good?).  could i solicit\\nsome opinions of people who use the 160 and 180 day-to-day on if its worth\\ntaking the disk size and money hit to get the active display?  (i realize\\nthis is a real subjective question, but i\\'ve only played around with the\\nmachines in a computer store breifly and figured the opinions of somebody\\nwho actually uses the machine daily might prove helpful).\\n\\n* how well does hellcats perform?  ;)\\n\\nthanks a bunch in advance for any info - if you could email, i\\'ll post a\\nsummary (news reading time is at a premium with finals just around the\\ncorner... :( )\\n--\\nTom Willis  \\\\  twillis@ecn.purdue.edu    \\\\    Purdue Electrical Engineering\\n---------------------------------------------------------------------------\\n\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\\nNietzsche\\n', 'From: jgreen@amber (Joe Green)\\nSubject: Re: Weitek P9000 ?\\nOrganization: Harris Computer Systems Division\\nLines: 14\\nDistribution: world\\nNNTP-Posting-Host: amber.ssd.csd.harris.com\\nX-Newsreader: TIN [version 1.1 PL9]\\n\\nRobert J.C. Kyanko (rob@rjck.UUCP) wrote:\\n> abraxis@iastate.edu writes in article <abraxis.734340159@class1.iastate.edu>:\\n> > Anyone know about the Weitek P9000 graphics chip?\\n> As far as the low-level stuff goes, it looks pretty nice.  It\\'s got this\\n> quadrilateral fill command that requires just the four points.\\n\\nDo you have Weitek\\'s address/phone number?  I\\'d like to get some information\\nabout this chip.\\n\\n--\\nJoe Green\\t\\t\\t\\tHarris Corporation\\njgreen@csd.harris.com\\t\\t\\tComputer Systems Division\\n\"The only thing that really scares me is a person with no sense of humor.\"\\n\\t\\t\\t\\t\\t\\t-- Jonathan Winters\\n', 'From: jcm@head-cfa.harvard.edu (Jonathan McDowell)\\nSubject: Re: Shuttle Launch Question\\nOrganization: Smithsonian Astrophysical Observatory, Cambridge, MA,  USA\\nDistribution: sci\\nLines: 23\\n\\nFrom article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\\n>>In article <C5JLwx.4H9.1@cs.cmu.edu>, ETRAT@ttacs1.ttu.edu (Pack Rat) writes...\\n>>>\"Clear caution & warning memory.  Verify no unexpected\\n>>>errors. ...\".  I am wondering what an \"expected error\" might\\n>>>be.  Sorry if this is a really dumb question, but\\n> \\n> Parity errors in memory or previously known conditions that were waivered.\\n>    \"Yes that is an error, but we already knew about it\"\\n> I\\'d be curious as to what the real meaning of the quote is.\\n> \\n> tom\\n\\n\\nMy understanding is that the \\'expected errors\\' are basically\\nknown bugs in the warning system software - things are checked\\nthat don\\'t have the right values in yet because they aren\\'t\\nset till after launch, and suchlike. Rather than fix the code\\nand possibly introduce new bugs, they just tell the crew\\n\\'ok, if you see a warning no. 213 before liftoff, ignore it\\'.\\n\\n - Jonathan\\n\\n\\n']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data['data'][1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference: **\n",
    "\n",
    "- [CountVectorizer documenttaion](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "- [CountVectorizer example](http://adataanalyst.com/scikit-learn/countvectorizer-sklearn-example/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Given the `CountVectorizer` we can see the number of words in our _bag_ as well as the number of documents on which we train, which in this case agrees with the values we obtained when we read in the data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a vocabulary from our data\n",
    "cv.fit(text_data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\n",
      "Subject: Diamond SS24X, Win 3.1, Mouse cursor\n",
      "Organization: National Library of Medicine\n",
      "Lines: 10\n",
      "\n",
      "\n",
      "Anybody seen mouse cursor distortion running the Diamond 1024x768x256 driver?\n",
      "Sorry, don't know the version of the driver (no indication in the menus) but it's a recently\n",
      "delivered Gateway system.  Am going to try the latest drivers from Diamond BBS but wondered\n",
      "if anyone else had seen this.\n",
      "\n",
      "post or email\n",
      "\n",
      "--Don Lindbergh\n",
      "dabl2@lhc.nlm.nih.gov\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference: **\n",
    "\n",
    "- [Working with text - creating Document Term Matrix](https://de.dariah.eu/tatom/working_with_text.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now process documents.\n",
    "\n",
    "# We need an iteratable to apply cv.transform()\n",
    "msg = []\n",
    "msg.append(message)\n",
    "\n",
    "# Transforming a single message is easier to comprehend. \n",
    "# By default, scikit learn uses sparse matrices for text processing\n",
    "# It returns a Document Term Matrix (dtm)\n",
    "dtm = cv.transform(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\nSubject: Diamond SS24X, Win 3.1, Mouse cursor\\nOrganization: National Library of Medicine\\nLines: 10\\n\\n\\nAnybody seen mouse cursor distortion running the Diamond 1024x768x256 driver?\\nSorry, don't know the version of the driver (no indication in the menus) but it's a recently\\ndelivered Gateway system.  Am going to try the latest drivers from Diamond BBS but wondered\\nif anyone else had seen this.\\n\\npost or email\\n\\n--Don Lindbergh\\ndabl2@lhc.nlm.nih.gov\\n\"]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 57 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples = 1\n",
      "Number of Tokens = 130107\n",
      "--------------------------------------------------------------------------------\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "--------------------------------------------------------------------------------\n",
      "Cells from Document-Term Matrix[i, j] and c (Count)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# In sparse format number of tokens indicate size of dataset vocabulary. \n",
    "# So there is 1 document and 130107 featues in the dtm.\n",
    "print('Number of Samples = {0}'.format(dtm.shape[0]))\n",
    "print('Number of Tokens = {0}'.format(dtm.shape[1]))\n",
    "print(80*'-')\n",
    "\n",
    "\n",
    "# You can't explore the document-term matrix when it is in sparse form. \n",
    "# We can convert from sparse to dense form to explore \n",
    "# the document-term matrix. The range given below is chosen randomly. \n",
    "# Each word is a feature. Below zeros indicate the words/features \n",
    "# in columns 1000 to 1100, those words does not appear in \n",
    "# the input message. Thats why we have zeros for those cells\n",
    "print(dtm.todense()[:,1000:1100])\n",
    "print(80*'-')\n",
    "\n",
    "\n",
    "# We can also print only nonzero DTM matrix elements. \n",
    "print('Cells from Document-Term Matrix[i, j] and c (Count)')\n",
    "print(80*'-')\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Find non-zero elements. scipy.sparse.find() returns the \n",
    "# indices and values of the nonzero elements of a matrix.\n",
    "# i,j contains the row and column indices where non zero matrix \n",
    "# entries are present while V has the entry's value.\n",
    "i, j, V = sp.find(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i #rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2336,   2643,  27721,  28604,  28615,  32185,  35805,  44124,\n",
       "        44830,  46126,  47059,  47961,  48546,  49055,  49057,  51215,\n",
       "        51268,  56979,  58245,  59686,  59893,  61546,  65798,  66608,\n",
       "        66894,  68766,  73201,  74705,  75589,  75742,  75990,  76032,\n",
       "        80816,  81150,  83955,  86155,  87284,  87546,  87626,  89362,\n",
       "        90252,  90379,  95136,  99966, 103523, 105843, 108930, 109967,\n",
       "       111322, 112674, 114455, 114731, 115475, 116722, 121234, 124397,\n",
       "       124923], dtype=int32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j #columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 3, 1, 3, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2,\n",
       "       1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V #row-column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ (0, 2336, 1), (0, 2643, 1), (0, 27721, 1), (0, 28604, 1), (0, 28615, 1),\n",
      "  (0, 32185, 1), (0, 35805, 2), (0, 44124, 2), (0, 44830, 2), (0, 46126, 1),\n",
      "  (0, 47059, 3), (0, 47961, 1), (0, 48546, 3), (0, 49055, 2), (0, 49057, 1),\n",
      "  (0, 51215, 1), (0, 51268, 1), (0, 56979, 2), (0, 58245, 1), (0, 59686, 1),\n",
      "  (0, 59893, 2), (0, 61546, 1), (0, 65798, 1), (0, 66608, 1), (0, 66894, 1),\n",
      "  (0, 68766, 1), (0, 73201, 1), (0, 74705, 1), (0, 75589, 1), (0, 75742, 1),\n",
      "  (0, 75990, 2), (0, 76032, 1), (0, 80816, 1), (0, 81150, 1), (0, 83955, 2),\n",
      "  (0, 86155, 1), (0, 87284, 2), (0, 87546, 2), (0, 87626, 1), (0, 89362, 2),\n",
      "  (0, 90252, 1), (0, 90379, 1), (0, 95136, 1), (0, 99966, 1), (0, 103523, 1),\n",
      "  (0, 105843, 2), (0, 108930, 1), (0, 109967, 1), (0, 111322, 1),\n",
      "  (0, 112674, 1), (0, 114455, 5), (0, 114731, 1), (0, 115475, 1),\n",
      "  (0, 116722, 1), (0, 121234, 1), (0, 124397, 1), (0, 124923, 1)]\n"
     ]
    }
   ],
   "source": [
    "dtm_list = list(zip(i, j, V))\n",
    "pp.pprint(dtm_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Look at the documentation for [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for what it returns. One of the things it returns is vocabulary_ which is a dictionary. Its a mapping of terms to feature indices/columns in dtm. So if you look at below cell it will return the column number in dtm where confuse exists as a column/feature in dtm. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41911\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_[\"confuse\"])\n",
    "# returns the column number of the feature column \"confuse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**itemgetter()** returns an object that in turn fetches an item from its operand. If multiple items are specified, it returns a tuple of lookup values. Look at the documentation for [itemgetter()](https://docs.python.org/2/library/operator.html#). Search for itemgetter in the page. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('B', 'D', 'F')\n",
      "===========================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('orange', 1), ('banana', 2), ('apple', 3), ('pear', 5)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "print(itemgetter(1,3,5)('ABCDEFG'))\n",
    "\n",
    "\n",
    "# Lets look at another example on how itemgetter() words. \n",
    "# inventory is a list with 4 tuples\n",
    "inventory = [('apple', 3), ('banana', 2), ('pear', 5), ('orange', 1)]\n",
    "\n",
    "# getcount has an object that will fetch items at index 1 on what ever it is called upon.\n",
    "getcount = itemgetter(1)\n",
    "\n",
    "print(\"===========================\")\n",
    "\n",
    "# Since key=getcount, get count will fetch the \n",
    "# values 3,2,1,4 which are present at index 1 in inventory.\n",
    "# Based on this keys the items in inventory are sorted\n",
    "sorted(inventory, key=getcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can grab the words in our _bag of words_ by extracting the _vocabulary_. This allows us to see if words are present in the documents. We have created a list dtm_list with the frequencies of words that appeared in dtn and their row column indices. Below code cell prints the first 5 elemnets in the list. \n",
    "\n",
    "We can also find which term occurs most frequently, least frequently, as well as the overall top terms in following cells. \n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2336, 1), (0, 2643, 1), (0, 27721, 1), (0, 28604, 1), (0, 28615, 1)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 114455, 5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(dtm_list, key=itemgetter(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen Word (confuse): Column = 41911\n",
      "-------------------\n",
      "['the']\n",
      "['10']\n",
      "-------------------\n",
      "Max Word (the): Column = 114455\n",
      "Min Word (10): Column = 2336\n"
     ]
    }
   ],
   "source": [
    "# Explore the terms in the vocabulary\n",
    "terms = cv.vocabulary_\n",
    "\n",
    "# Look for a single term confuse\n",
    "search_word = 'confuse'\n",
    "print(\"Chosen Word ({0}): Column = {1}\".format(search_word, terms[search_word]))\n",
    "\n",
    "# Find the maximum value in dtm_list in 3rd column which will be 114455\n",
    "max_key = max(dtm_list, key=itemgetter(2))[1]\n",
    "\n",
    "# Find the minimum value in dtm_list in 3rd column which will be 2336\n",
    "min_key = min(dtm_list, key=itemgetter(2))[1]\n",
    "\n",
    "# In below two lines, terms.keys() will return all keys i'e the column names(words).\n",
    "# For loop iterates over all this words to see get the column name \n",
    "# which matches the column index we have in max_key and min_key\n",
    "x_max = [key for key in terms.keys() if terms[key] == max_key]\n",
    "x_min = [key for key in terms.keys() if terms[key] == min_key]\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(x_max)\n",
    "print(x_min)\n",
    "print(\"-------------------\")\n",
    "\n",
    "# for loop above returned a list as output. So x_max is a list with a column name as same with x_min\n",
    "print(\"Max Word ({0}): Column = {1}\".format(x_max[0], max_key))\n",
    "print(\"Min Word ({0}): Column = {1}\".format(x_min[0], min_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## NLTK\n",
    "\n",
    "The scikit learn library can help us do general purpose basic text analysis. But text analysis in itself is an extremely large and growing topic. To handle advanced text analysis, we will use a library called Natural Language ToolKit or [NLTK](http://www.nltk.org). It enables a wide range of text analyses either on its own, or in conjunction with scikit learn \n",
    "\n",
    "Refer to the [NLTK documentation](http://www.nltk.org/book/ch01.html). Focus on sections 2, 3, 5, and 6 from Chapter 1. These sections cover most of the topics that are demonstrated above. We will explore how to use NLTK to perform basic text analysis in next few cells. Import the library and tokenize the message. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell Run this cell once to download the nltk datasets. \n",
    "# Once the data is downloaded you can comment out \n",
    "# this cell or delete it. \n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from', ':', 'dabl2', '@', 'nlm.nih.gov', '(', 'don', 'a.b', '.', 'lindbergh', ')', 'subject', ':', 'diamond', 'ss24x', ',', 'win', '3.1', ',', 'mouse', 'cursor', 'organization', ':', 'national', 'library', 'of', 'medicine', 'lines', ':', '10', 'anybody', 'seen', 'mouse', 'cursor', 'distortion', 'running', 'the', 'diamond', '1024x768x256', 'driver', '?', 'sorry', ',', 'do', \"n't\", 'know', 'the', 'version', 'of', 'the', 'driver', '(', 'no', 'indication', 'in', 'the', 'menus', ')', 'but', 'it', \"'s\", 'a', 'recently', 'delivered', 'gateway', 'system', '.', 'am', 'going', 'to', 'try', 'the', 'latest', 'drivers', 'from', 'diamond', 'bbs', 'but', 'wondered', 'if', 'anyone', 'else', 'had', 'seen', 'this', '.', 'post', 'or', 'email', '--', 'don', 'lindbergh', 'dabl2', '@', 'lhc.nlm.nih.gov']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize a text document\n",
    "# word_tokenize() is tokenizing the message and each word is being converted to lowercase. \n",
    "# So words has the vocabulary of message\n",
    "words = [word.lower() for word in nltk.word_tokenize(message)]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 69 samples and 95 outcomes>\n"
     ]
    }
   ],
   "source": [
    "top_display=10\n",
    "\n",
    "# Count number of occurances for each token\n",
    "counts = nltk.FreqDist(words)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ('the', 5), (':', 4), ('diamond', 3), ('.', 3), (',', 3), ('@', 2),\n",
      "  ('seen', 2), ('driver', 2), ('(', 2), ('of', 2)]\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(counts.most_common(top_display))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "**nltk.FreqDist() :**  This function gives us the frequency of each vocabulary item in the text. It is a \"distribution\" because it tells us how the total number of word tokens in the text are distributed across the vocabulary items.\n",
    "\n",
    "\n",
    "**Regular Expressions: ** We can clean up the list of tokens by using a regular expression with the `word_tokenize` method. One benefit of using regular expressions is that we can specifically indicate of what a token should be composed, in this case, we state a token is a sequence of one or more alphanumeric characters surrounded by white space. The regular expression ^\\w\\s identifies tokens as one or more alphanumeric characters followed by a whitespace character. Doing this removes the punctuation tokens, as shown below. You can compare the output of previous code cell and next cell. The symbols (':', '@', '.', ',', '(') are no more counted as tokens.\n",
    "\n",
    "^\\w\\s\n",
    "\n",
    "- ^ - asserts position at start of the string\n",
    "\n",
    "- \\w - matches any word character (equal to [a-zA-Z0-9_])\n",
    "\n",
    "- \\s - matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ('the', 5), ('diamond', 3), ('don', 3), ('but', 2), ('seen', 2),\n",
      "  ('driver', 2), ('nlm', 2), ('nih', 2), ('a', 2), ('of', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Specify a Regular Expression to parse a text document\n",
    "import re\n",
    "pattern = re.compile(r'[^\\w\\s]')\n",
    "words = [word.lower() for word in nltk.word_tokenize(re.sub(pattern, ' ', message))]\n",
    "\n",
    "# Count token occurances\n",
    "counts = nltk.FreqDist(words)\n",
    "pp.pprint(counts.most_common(top_display))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "You can measure the diversity of terms which is the fraction of unique tokens, or terms, in a document to the total tokens or terms in a document. [lexical diversity](https://en.wikipedia.org/wiki/Lexical_diversity). \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message has 63 tokens and 84 words for a lexical diversity of 1.333\n"
     ]
    }
   ],
   "source": [
    "num_words = len(words)\n",
    "num_tokens = len(counts)\n",
    "lexdiv  =  num_words / num_tokens\n",
    "print(\"Message has %i tokens and %i words for a lexical diversity of %0.3f\" % (num_tokens, num_words, lexdiv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can compute the number of [unique sample values](http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist.B), [number of samples outcomes](http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist.N), and the [maximum occurring token](http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist.N) with simple NLTK statistical functions. We can also iterate through and display the most commonly occurring terms and their counts.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique bins(tokens) = 63\n",
      "\n",
      "Number of sample outcomes = 84\n",
      "\n",
      "Maximum occuring token = the\n",
      "\n",
      "Term        : Count\n",
      "-------------------------\n",
      "the         :  5.000\n",
      "diamond     :  3.000\n",
      "don         :  3.000\n",
      "but         :  2.000\n",
      "seen        :  2.000\n",
      "driver      :  2.000\n",
      "nlm         :  2.000\n",
      "nih         :  2.000\n",
      "a           :  2.000\n",
      "of          :  2.000\n"
     ]
    }
   ],
   "source": [
    "# Display number of unique tokens (or bins)\n",
    "print('Number of unique bins(tokens) = {0}\\n'.format(counts.B()))\n",
    "print('Number of sample outcomes = {0}\\n'.format(counts.N()))\n",
    "print('Maximum occuring token = {0}\\n'.format(counts.max()))\n",
    "\n",
    "print('{0:12s}: {1}'.format('Term', 'Count'))\n",
    "print(25*'-')\n",
    "\n",
    "for token, freq in counts.most_common(top_display):\n",
    "    print('{0:12s}:  {1:4.3f}'.format(token, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Words that occur rarely are important sometimes. For example, in a classification process, words that are uniquely assigned to a particular message should carry more weight. Words that only occur once in an entire set of documents, or corpus, provide unique insight into the particular text document in which they occur. A word that only occurs once in an entire corpus is known as a [_hapax_](https://en.wikipedia.org/wiki/Hapax_legomenon). NLTK has a `hapaxes` method that can be used to quickly find _hapaxes_ in a corpus.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'going', 'am', 'anyone', 's', 'drivers', 'gateway', 'had', 'subject', 'it',\n",
      "  'system']\n"
     ]
    }
   ],
   "source": [
    "# Hapaxes\n",
    "pp.pprint(counts.hapaxes()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "NLTK can be used to see the most commonly used tokens. `tabulate` method can display the top tokens and their frequency. Once the counts are displayed, we will visually plot the counts of the top tokens by using the `plot` method.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    the diamond     don     but    seen  driver     nlm     nih       a      of \n",
      "      5       3       3       2       2       2       2       2       2       2 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure object at 0x7fc0a237cc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number of elements to display\n",
    "top_display=10\n",
    "counts.tabulate(top_display)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(10,6))\n",
    "sns.set(style=\"white\", font_scale=1.5)\n",
    "sns.despine(offset=5)#, trim=True)\n",
    "counts.plot(top_display, cumulative=True)\n",
    "axs.set_title('Term Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try cumulative=True in above plot and re run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## NLTK Corpus\n",
    "\n",
    "The NLTK library includes a number of [data sets](http://www.nltk.org/nltk_data/) that can be downloaded and used directly from within NLTK. Lets use the NLTK [movie review corpus](http://www.cs.cornell.edu/people/pabo/movie-review-data/). \n",
    "\n",
    "In the following code cells, we access the movie review data set, display (part) of the data set's README (or general documentation), before we begin to process the words or terms in the corpus.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the NLTK movie review data set. \n",
    "\n",
    "mvr = nltk.corpus.movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Polarity Dataset Version 2.0\n",
      "Bo Pang and Lillian Lee\n",
      "\n",
      "http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
      "\n",
      "Distributed with NLTK with permission from the authors.\n",
      "\n",
      "=======\n",
      "\n",
      "Introduction\n"
     ]
    }
   ],
   "source": [
    "# Print the data set README\n",
    "print(mvr.readme()[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie Review has 39768 tokens and 1583820 words for a lexical diversity of 39.826\n"
     ]
    }
   ],
   "source": [
    "mvr_words = mvr.words()\n",
    "counts  = nltk.FreqDist(mvr_words)\n",
    "num_words = len(mvr_words)\n",
    "num_tokens = len(counts)\n",
    "lexdiv  =  num_words / num_tokens\n",
    "print(\"Movie Review has {0} tokens and {1} words for a lexical diversity of {2:4.3f}\".format(num_tokens, num_words, lexdiv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party',\n",
      "  ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an',\n",
      "  'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his',\n",
      "  'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',',\n",
      "  'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?',\n",
      "  'watch']\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(mvr.words()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The data are organized into separate files for each movie review. Since these reviews have an associated sentiment:? negative and positive, the reviews are categorized (via a directory structure) into `neg` or `pos` respectively. We can directly access a single review, which can be treated as a single text document. In the next few code cells we directly access the number of files, which can be used to count the number of reviews (assuming one review per file). We also display the contents of a single file, before displaying a subset of the files in one particular category, in this case `neg`, or negative reviews.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of reviews = 2000\n"
     ]
    }
   ],
   "source": [
    "# Each article is in a separate file\n",
    "\n",
    "print('Total Number of reviews = {0}'.format(len(mvr.fileids())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example File: neg/cv000_29416.txt\n"
     ]
    }
   ],
   "source": [
    "a_filename = mvr.fileids()[0]\n",
    "print('Example File: {0}'.format(a_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display first 199 charcaters of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('plot : two teen couples go to a church party , drink and then drive . \\n'\n",
      " 'they get into an accident . \\n'\n",
      " 'one of the guys dies , but his girlfriend continues to see him in her life '\n",
      " ', and has nightmares . \\n')\n"
     ]
    }
   ],
   "source": [
    "# Print part of the file\n",
    "pp.pprint(mvr.raw(a_filename)[:199])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "# Display article assigned categories\n",
    "pp.pprint(mvr.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt',\n",
      "  'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt',\n",
      "  'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt',\n",
      "  'neg/cv009_29417.txt', 'neg/cv010_29063.txt', 'neg/cv011_13044.txt',\n",
      "  'neg/cv012_29411.txt', 'neg/cv013_10494.txt', 'neg/cv014_15600.txt',\n",
      "  'neg/cv015_29356.txt', 'neg/cv016_4348.txt', 'neg/cv017_23487.txt',\n",
      "  'neg/cv018_21672.txt', 'neg/cv019_16117.txt']\n"
     ]
    }
   ],
   "source": [
    "# Find first 20 articles that have negative category\n",
    "pp.pprint(mvr.fileids('neg')[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Given the contents of a file, we can process the associated text in the same manner as before. In this case, we tokenize one review into sentences as opposed to the traditional word tokens. The sents() function divides the text up into its sentences, where each sentence is a list of words:\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ '9', ':', 'its', 'pathetic', 'attempt', 'at', '\"', 'improving', '\"', 'on',\n",
      "  'a', 'shakespeare', 'classic', '.']\n",
      "['8', ':', 'its', 'just', 'another', 'piece', 'of', 'teen', 'fluff', '.']\n",
      "['7', ':', 'kids', 'in', 'high', 'school', 'are', 'not', 'that', 'witty', '.']\n",
      "['6', ':', 'the', 'wittiness', 'is', 'not', 'witty', 'enough', '.']\n",
      "['5', ':', 'the', 'comedy', 'is', 'not', 'funny', '.']\n",
      "['4', ':', 'the', 'acting', 'is', 'poor', '.']\n",
      "['3', ':', 'the', 'music', '.']\n",
      "['2', ':', 'the', 'poster', '.']\n",
      "['1', ':', 'its', 'worse', 'than', 'she', \"'\", 's', 'all', 'that', '!']\n",
      "[ '10', '=', 'a', 'classic', '9', '=', 'borderline', 'classic', '8', '=',\n",
      "  'excellent', '7', '=', 'good', '6', '=', 'better', 'than', 'average', '5',\n",
      "  '=', 'average', '4', '=', 'disappointing', '3', '=', 'poor', '2', '=',\n",
      "  'awful', '1', '=', 'a', 'crap', 'classic']\n"
     ]
    }
   ],
   "source": [
    "# Display sentances from an article\n",
    "\n",
    "a_filename = 'neg/cv779_18989.txt'\n",
    "for sent in mvr.sents(a_filename):\n",
    "    pp.pprint(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are listing out words that are much longer than normal. As this simple example demonstrates, this can be a useful technique to search for potential problems, since in this case, none of the example shown are actual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'uuuuuuggggggglllllllyyyyy', 's_funniest_home_videos_',\n",
      "  '_the_last_days_of_disco_', '_i_know_what_you_did_last_summer_',\n",
      "  '_fear_and_loathing_in_las_vegas_', '_breakfast_of_champions_',\n",
      "  '_breakfast_of_champions_', '_a_night_at_the_roxbury_',\n",
      "  '_a_night_at_the_roxbury_',\n",
      "  '__________________________________________________________',\n",
      "  '____________________________________________', '==========================',\n",
      "  '========================', '=======================',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------']\n"
     ]
    }
   ],
   "source": [
    "# We can process the words with normal Python\n",
    "# For example, print out really long words\n",
    "long_words = [word for word in mvr_words if len(word) > 22]\n",
    "long_words.sort(reverse=True)\n",
    "pp.pprint(long_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
