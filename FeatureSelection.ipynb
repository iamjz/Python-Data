{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection reduces the dimensionalit of data for the following reasons:\n",
    "- Reduces overfitting by removing noise introduced by some of the features\n",
    "- Reduces training time, which allows you to experiment more with different models and hyperparameters\n",
    "- Reduces data acquisition requirements\n",
    "- Improves comprehensibility of the model because a smaller set of features is more comprehendible to humans. This enables us to focus on the main sources of predictability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection methods generally fall into 2 categories. Filter Methods and Wrapper Methods. \n",
    "\n",
    "- Filter Methods: Apply a statistical measure and assign a score to each feature one at a time. Pearson's X2 and ANOVA F-Value based feature selection. \n",
    "\n",
    "- Wrapper Methods: Use a subset of features. Based on the results drawn from the previous model trained on that subset of features, they are either added or removed from the subset. The problem is essentially reduced to a search problem. Greedy algos (https://en.wikipedia.org/wiki/Greedy_algorithm) are the most desirable in multivariate feature selection scenarios because the wrapper methods are usually computationally very expensive and greedy algos don't necessarily provide the optimal solution, which is a good thing because it makes them less prone to overfitting. Forward Selection, Backward Elimination, Recursive Feature Elimination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.base import clone\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "datasource = \"datasets/winequality-red.csv\"\n",
    "print(os.path.exists(datasource))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.069</td>\n",
       "      <td>35.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.99632</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.081</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.99564</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.60</td>\n",
       "      <td>10.3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.2</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.081</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.00060</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.99750</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.61</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.7</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.112</td>\n",
       "      <td>11.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.99630</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.3             0.690         0.32             2.2      0.069   \n",
       "1            7.2             0.570         0.05             2.3      0.081   \n",
       "2           13.2             0.380         0.55             2.7      0.081   \n",
       "3            9.0             0.530         0.49             1.9      0.171   \n",
       "4            7.7             0.965         0.10             2.1      0.112   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 35.0                 104.0  0.99632  3.33       0.51   \n",
       "1                 16.0                  36.0  0.99564  3.38       0.60   \n",
       "2                  5.0                  16.0  1.00060  2.98       0.54   \n",
       "3                  6.0                  25.0  0.99750  3.27       0.61   \n",
       "4                 11.0                  22.0  0.99630  3.26       0.50   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.5        5  \n",
       "1     10.3        6  \n",
       "2      9.4        5  \n",
       "3      9.4        6  \n",
       "4      9.5        5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(datasource).sample(frac = 1).reset_index(drop = True)\n",
    "del df[\"Unnamed: 0\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(df.iloc[:, :-1])\n",
    "y = np.array(df[\"quality\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection solution space\n",
    "\n",
    "From algorithm analysus' point of view, a solution for feature selection problems can be represented as a boolean vector, each component indicating whether the corresponding feature has been selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True False False  True  True False False False  True]\n"
     ]
    }
   ],
   "source": [
    "selected = np.array([False, True, True, False, False, True, True, False, False, False, True])\n",
    "print(selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn calls the corresponding indices to feature columns selected \"support\", which can be obtained using np.flatnonzero(). \n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.flatnonzero.html\n",
    "\n",
    "Return indices that are non-zero in the flattened version of a.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  5  6 10]\n"
     ]
    }
   ],
   "source": [
    "support = np.flatnonzero(selected)\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, a naive approach that exhaustively search all subsets of features would have to verify 2^p solutions. This is very inefficient. However, we will run an exhaustive search for all solutions that provide 5 features to establish a baseline. This limits our time complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.35149423850184036, (1, 4, 6, 9, 10)),\n",
       " (0.34831403567393349, (1, 4, 8, 9, 10)),\n",
       " (0.34695788821029638, (1, 6, 8, 9, 10))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search_combinations(estimator, X, y, k = 5):\n",
    "    # fit and score model based on some subset of features\n",
    "    score = lambda X_features: clone(estimator).fit(X_features, y).score(X_features, y)\n",
    "    \n",
    "    # enumerate all combinations of 5 features\n",
    "    for subset in itertools.combinations(range(X.shape[1]), 5):\n",
    "        yield score(X[:, subset]), subset\n",
    "        \n",
    "sorted(search_combinations(LinearRegression(), X, y), reverse = True)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
