{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='build_it' ></a>\n",
    "\n",
    "## Buiding our Whoosh Schema\n",
    "\n",
    "Recall, the `book/` folder is composed of a collection of text files, each its own book chapter.\n",
    "\n",
    "In whoosh, structure the retrieval system by defining a storage schema.\n",
    "\n",
    "```\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                content=TEXT(analyzer=StemmingAnalyzer())\n",
    "                )\n",
    "```\n",
    "\n",
    "This tells us we are defining records to have a `(filename, content, tags)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                line_num=ID(stored=True),\n",
    "                content=TEXT(analyzer=StemmingAnalyzer(),stored=True)\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='load_it' ></a>\n",
    "\n",
    "## Loading Data\n",
    "\n",
    "The books are in the folder called books in `lab/` folder:\n",
    "\n",
    "Create the _whoosh_ index files in the folder, then ingest the files.\n",
    "\n",
    "To load the data, a python script with follow the basic crawling behavior\n",
    "\n",
    " 1. For each file/folder in the specified starting folder:\n",
    " 1. If it is a folder, recurse into folder and process contents\n",
    " 1. If it is a file, read contents and load into indexer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "from whoosh import index\n",
    "\n",
    "# Note, this clears the existing index in the directory\n",
    "ix = index.create_in(\"indexes\", schema)\n",
    "\n",
    "# Get a writer form the created index in \n",
    "writer = ix.writer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder:  book\n",
      "root =  book\n",
      "Processing File: book/1chron.txt\n",
      "Indexed:  book/1chron.txt\n",
      "Processing File: book/1corinth.txt\n",
      "Indexed:  book/1corinth.txt\n",
      "Processing File: book/1john.txt\n",
      "Indexed:  book/1john.txt\n",
      "Processing File: book/1kings.txt\n",
      "Indexed:  book/1kings.txt\n",
      "Processing File: book/1peter.txt\n",
      "Indexed:  book/1peter.txt\n",
      "Processing File: book/1samuel.txt\n",
      "Indexed:  book/1samuel.txt\n",
      "Processing File: book/1thess.txt\n",
      "Indexed:  book/1thess.txt\n",
      "Processing File: book/1timothy.txt\n",
      "Indexed:  book/1timothy.txt\n",
      "Processing File: book/2chron.txt\n",
      "Indexed:  book/2chron.txt\n",
      "Processing File: book/2corinth.txt\n",
      "Indexed:  book/2corinth.txt\n",
      "Processing File: book/2john.txt\n",
      "Indexed:  book/2john.txt\n",
      "Processing File: book/2kings.txt\n",
      "Indexed:  book/2kings.txt\n",
      "Processing File: book/2peter.txt\n",
      "Indexed:  book/2peter.txt\n",
      "Processing File: book/2samuel.txt\n",
      "Indexed:  book/2samuel.txt\n",
      "Processing File: book/2thess.txt\n",
      "Indexed:  book/2thess.txt\n",
      "Processing File: book/2timothy.txt\n",
      "Indexed:  book/2timothy.txt\n",
      "Processing File: book/3john.txt\n",
      "Indexed:  book/3john.txt\n",
      "Processing File: book/acts.txt\n",
      "Indexed:  book/acts.txt\n",
      "Processing File: book/amos.txt\n",
      "Indexed:  book/amos.txt\n",
      "Processing File: book/colossia.txt\n",
      "Indexed:  book/colossia.txt\n",
      "Processing File: book/daniel.txt\n",
      "Indexed:  book/daniel.txt\n",
      "Processing File: book/deut.txt\n",
      "Indexed:  book/deut.txt\n",
      "Processing File: book/eccl.txt\n",
      "Indexed:  book/eccl.txt\n",
      "Processing File: book/ephesian.txt\n",
      "Indexed:  book/ephesian.txt\n",
      "Processing File: book/esther.txt\n",
      "Indexed:  book/esther.txt\n",
      "Processing File: book/exodus.txt\n",
      "Indexed:  book/exodus.txt\n",
      "Processing File: book/ezekiel.txt\n",
      "Indexed:  book/ezekiel.txt\n",
      "Processing File: book/ezra.txt\n",
      "Indexed:  book/ezra.txt\n",
      "Processing File: book/galatian.txt\n",
      "Indexed:  book/galatian.txt\n",
      "Processing File: book/genesis.txt\n",
      "Indexed:  book/genesis.txt\n",
      "Processing File: book/habakkuk.txt\n",
      "Indexed:  book/habakkuk.txt\n",
      "Processing File: book/haggai.txt\n",
      "Indexed:  book/haggai.txt\n",
      "Processing File: book/hebrews.txt\n",
      "Indexed:  book/hebrews.txt\n",
      "Processing File: book/hosea.txt\n",
      "Indexed:  book/hosea.txt\n",
      "Processing File: book/isaiah.txt\n",
      "Indexed:  book/isaiah.txt\n",
      "Processing File: book/james.txt\n",
      "Indexed:  book/james.txt\n",
      "Processing File: book/jeremiah.txt\n",
      "Indexed:  book/jeremiah.txt\n",
      "Processing File: book/job.txt\n",
      "Indexed:  book/job.txt\n",
      "Processing File: book/joel.txt\n",
      "Indexed:  book/joel.txt\n",
      "Processing File: book/john.txt\n",
      "Indexed:  book/john.txt\n",
      "Processing File: book/jonah.txt\n",
      "Indexed:  book/jonah.txt\n",
      "Processing File: book/joshua.txt\n",
      "Indexed:  book/joshua.txt\n",
      "Processing File: book/jude.txt\n",
      "Indexed:  book/jude.txt\n",
      "Processing File: book/judges.txt\n",
      "Indexed:  book/judges.txt\n",
      "Processing File: book/lament.txt\n",
      "Indexed:  book/lament.txt\n",
      "Processing File: book/levit.txt\n",
      "Indexed:  book/levit.txt\n",
      "Processing File: book/luke.txt\n",
      "Indexed:  book/luke.txt\n",
      "Processing File: book/malachi.txt\n",
      "Indexed:  book/malachi.txt\n",
      "Processing File: book/mark.txt\n",
      "Indexed:  book/mark.txt\n",
      "Processing File: book/matthew.txt\n",
      "Indexed:  book/matthew.txt\n",
      "Processing File: book/micah.txt\n",
      "Indexed:  book/micah.txt\n",
      "Processing File: book/nahum.txt\n",
      "Indexed:  book/nahum.txt\n",
      "Processing File: book/nehemiah.txt\n",
      "Indexed:  book/nehemiah.txt\n",
      "Processing File: book/numbers.txt\n",
      "Indexed:  book/numbers.txt\n",
      "Processing File: book/obadiah.txt\n",
      "Indexed:  book/obadiah.txt\n",
      "Processing File: book/philemon.txt\n",
      "Indexed:  book/philemon.txt\n",
      "Processing File: book/philipp.txt\n",
      "Indexed:  book/philipp.txt\n",
      "Processing File: book/proverbs.txt\n",
      "Indexed:  book/proverbs.txt\n",
      "Processing File: book/psalms.txt\n",
      "Indexed:  book/psalms.txt\n",
      "Processing File: book/rev.txt\n",
      "Indexed:  book/rev.txt\n",
      "Processing File: book/romans.txt\n",
      "Indexed:  book/romans.txt\n",
      "Processing File: book/ruth.txt\n",
      "Indexed:  book/ruth.txt\n",
      "Processing File: book/song.txt\n",
      "Indexed:  book/song.txt\n",
      "Processing File: book/titus.txt\n",
      "Indexed:  book/titus.txt\n",
      "Processing File: book/zech.txt\n",
      "Indexed:  book/zech.txt\n",
      "Processing File: book/zeph.txt\n",
      "Indexed:  book/zeph.txt\n",
      "recursing into  one_level_down\n",
      "Processing folder:  one_level_down\n",
      "root =  book/one_level_down\n",
      "recursing into  two_levels_down\n",
      "Processing folder:  two_levels_down\n",
      "root =  book/one_level_down/two_levels_down\n",
      "Processing File: book/one_level_down/two_levels_down/test.txt\n",
      "Indexed:  book/one_level_down/two_levels_down/test.txt\n"
     ]
    }
   ],
   "source": [
    "def loadFile(writer, fname):\n",
    "    '''\n",
    "    Read file contents, load into database.\n",
    "    '''\n",
    "    line_no = 1\n",
    "    with open(fname, 'r') as infile:\n",
    "        for line in infile:\n",
    "            line = line.rstrip('\\n')\n",
    "            line_no += 1\n",
    "            writer.add_document(filename=fname, \\\n",
    "                                line_num=str(line_no),\\\n",
    "                                content=line)\n",
    "    print(\"Indexed: \", fname)\n",
    "\n",
    "\n",
    "#     with open(fname, 'r') as infile:\n",
    "#         content=infile.read()\n",
    "#         txt = content.splitlines()\n",
    "#         for line in txt:\n",
    "#             writer.add_document(filename=fname, content=line)\n",
    "#         print(\"Indexed: \", fname)\n",
    "\n",
    "def processFolder(writer,folder):\n",
    "    '''\n",
    "    Process a folder for files and subfolders\n",
    "    '''\n",
    "    print('Processing folder: ',folder)\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        print(\"root = \", root)\n",
    "        # Process Files\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                filename = os.path.join(root, file)\n",
    "                print('Processing File:',filename)\n",
    "                loadFile(writer,filename)\n",
    "            else:\n",
    "                print(\"Unhandled File\")\n",
    "        # Recurse into subfolders\n",
    "        for d in dirs:\n",
    "            print(\"recursing into \",d)\n",
    "            processFolder(writer,d)\n",
    "\n",
    "# Functions defined,  get the party started:\n",
    "processFolder(writer,\"book\")\n",
    "writer.commit() # save changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='search_me' ></a>\n",
    "\n",
    "## Executing Queries\n",
    "\n",
    "Read: \n",
    "  http://whoosh.readthedocs.io/en/latest/searching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book/luke.txt\n",
      "book/john.txt\n",
      "book/1john.txt\n",
      "book/1john.txt\n",
      "book/1john.txt\n",
      "book/1john.txt\n",
      "book/john.txt\n",
      "book/1john.txt\n",
      "book/john.txt\n",
      "book/malachi.txt\n"
     ]
    }
   ],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "q = qp.parse(u\"love\")\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit[\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the results contains at most the first 10 matching documents. To get more results, use the limit keyword:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with ix.searcher() as s:\n",
    "    results = s.search(q, limit=20)\n",
    "    for hit in results:\n",
    "        print(hit[\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have set the limit for maximum of 20 results to return and above code will return 19 results. If you want all results, use limit=None. However, setting the limit whenever possible makes searches faster because Whoosh doesn’t need to examine and score every document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "<a id='Scoring' ></a>\n",
    "\n",
    "## Scoring\n",
    "\n",
    "Until this point you should be familiar with all the code above. The code cell above illustrates the search results using the vector space model. In coming cells, we will be using a scoring criteria while searching the indexes below. \n",
    "\n",
    "\n",
    "Normally the list of result documents is sorted by score. The whoosh.scoring module contains implementations of various scoring algorithms. The default is [BM25F](https://en.wikipedia.org/wiki/Okapi_BM25). You can set the scoring object to use when you create the searcher using the weighting keyword argument: \n",
    "\n",
    "````\n",
    "from whoosh import scoring\n",
    "\n",
    "with myindex.searcher(weighting=scoring.TF_IDF()) as s:\n",
    "    ... ````\n",
    "    \n",
    "    \n",
    "A weighting model is a WeightingModel subclass with a scorer() method that produces a “scorer” instance. This instance has a method that takes the current matcher and returns a floating point score.\n",
    "\n",
    "\n",
    "\n",
    "### TF IFD\n",
    "\n",
    "So why do we have to score the terms. Previously, we have simply used the number of times a token (i.e., word, or more generally an n-gram) occurs in a document to classify the document. Even with the removal of stop words, however, this can still overemphasize tokens that might generally occur across many documents (e.g., names or general concepts). An alternative technique that often provides robust improvements in classification accuracy is to employ the frequency of token occurrence, normalized over the frequency with which the token occurs in all documents. In this manner, we give higher weight in the classification process to tokens that are more strongly tied to a particular label. \n",
    "\n",
    "Formally this concept is known as [term frequency–inverse document frequency](https://en.wikipedia.org/wiki/Tf–idf) (or tf-idf). We will use this scoring method to compare the search results with normal vector space model.\n",
    "\n",
    "In below code cell, documents with a better TF-IDF score will appear higher in the search results list. Compare below results with the results of above cell which was using basic vector space model for scoring documents. Read the below documents to understand what TF_IDF nis about and how it is applied in whoosh. \n",
    " \n",
    "\n",
    "-----\n",
    "\n",
    "Reference: \n",
    "\n",
    "- [Scoring and sorting](http://whoosh.readthedocs.io/en/latest/searching.html#scoring-and-sorting)\n",
    "- [TF-IDF](http://www.tfidf.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book/luke.txt\n",
      "book/1john.txt\n",
      "book/1john.txt\n",
      "book/1john.txt\n",
      "book/1john.txt\n",
      "book/1samuel.txt\n",
      "book/hosea.txt\n",
      "book/john.txt\n",
      "book/john.txt\n",
      "book/malachi.txt\n"
     ]
    }
   ],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "from whoosh import scoring\n",
    "\n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "q = qp.parse(u\"love\")\n",
    "\n",
    "with ix.searcher(weighting=scoring.TF_IDF()) as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit[\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe the files **\"1samuel.txt\"** and **\"hosea.txt\"** have made it to top 10 while the file **john.txt** which was at position 6 and 7 is pushed down to positions 8 and 9 because of the TFIDF scores because of the ranking based on TDIDF scores. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Filtering results\n",
    "\n",
    "You can use the filter keyword argument in search() to specify a set of documents to permit in the results. The argument can be a whoosh.query.Query object, a whoosh.searching.Results object, or a set-like object containing document numbers. The searcher caches filters so if for example you use the same query filter with a searcher multiple times, the additional searches will be faster because the searcher will cache the results of running the filter query. You can also specify a mask keyword argument to specify a set of documents that are not permitted in the results. \n",
    "\n",
    "Lets first look up documents where hate is appearing.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book/deut.txt\n",
      "book/john.txt\n",
      "book/2samuel.txt\n",
      "book/proverbs.txt\n",
      "book/psalms.txt\n",
      "book/titus.txt\n",
      "book/1john.txt\n",
      "book/1kings.txt\n",
      "book/2chron.txt\n",
      "book/2chron.txt\n"
     ]
    }
   ],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "from whoosh import scoring\n",
    "\n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "q = qp.parse(u\"hate\")\n",
    "\n",
    "with ix.searcher(weighting=scoring.TF_IDF()) as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit[\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In below code cell, we are using filter argument to allow John.txt only in the results and mask the word hate. So if you observe the results below, indexes in john.txt have appeared and none of the indexes have hate in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book/john.txt 13:34: A new commandment I give unto you, That ye love one another; as I have loved you, that ye also love one another.\n",
      "book/john.txt 15:9: As the Father hath loved me, so have I loved you: continue ye in my love.\n",
      "book/john.txt 13:1: Now before the feast of the passover, when Jesus knew that his hour was come that he should depart out of this world unto the Father, having loved his own which were in the world, he loved them unto the end.\n",
      "book/john.txt 14:21: He that hath my commandments, and keepeth them, he it is that loveth me: and he that loveth me shall be loved of my Father, and I will love him, and will manifest myself to him.\n",
      "book/john.txt 14:23: Jesus answered and said unto him, If a man love me, he will keep my words: and my Father will love him, and we will come unto him, and make our abode with him.\n",
      "book/john.txt 15:10: If ye keep my commandments, ye shall abide in my love; even as I have kept my Father's commandments, and abide in his love.\n",
      "book/john.txt 15:12: This is my commandment, That ye love one another, as I have loved you.\n",
      "book/john.txt 17:23: I in them, and thou in me, that they may be made perfect in one; and that the world may know that thou hast sent me, and hast loved them, as thou hast loved me.\n",
      "book/john.txt 17:26: And I have declared unto them thy name, and will declare it: that the love wherewith thou hast loved me may be in them, and I in them.\n",
      "book/john.txt 3:16: For God so loved the world, that he gave his only begotten Son, that whosoever believeth in him should not perish, but have everlasting life.\n"
     ]
    }
   ],
   "source": [
    "from whoosh.query import *\n",
    "\n",
    "with ix.searcher(weighting=scoring.TF_IDF()) as s:\n",
    "    qp = QueryParser(\"content\", ix.schema)\n",
    "    user_q = qp.parse(u\"love\")\n",
    "\n",
    "    # Only show documents in the \"rendering\" chapter\n",
    "    allow_q = Term(\"filename\", \"book/john.txt\")\n",
    "    # Don't show any documents where the \"content\" field contains \"hate\"\n",
    "    restrict_q = Term(\"content\",\"hate\")\n",
    "\n",
    "    results = s.search(user_q, mask=restrict_q, filter=allow_q)      #   \n",
    "    for hit in results:\n",
    "        print(hit[\"filename\"], hit[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Lets put our results into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of matches:  363\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docnum</th>\n",
       "      <th>filename</th>\n",
       "      <th>line</th>\n",
       "      <th>line_num</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21820</td>\n",
       "      <td>book/luke.txt</td>\n",
       "      <td>6:32: For if ye love them which love you, what...</td>\n",
       "      <td>287</td>\n",
       "      <td>0</td>\n",
       "      <td>21.811635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1408</td>\n",
       "      <td>book/1john.txt</td>\n",
       "      <td>2:15: Love not the world, neither the things ...</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>16.358726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1456</td>\n",
       "      <td>book/1john.txt</td>\n",
       "      <td>4:10: Herein is love, not that we loved God, ...</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>16.358726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1462</td>\n",
       "      <td>book/1john.txt</td>\n",
       "      <td>4:16: And we have known and believed the love...</td>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>16.358726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1464</td>\n",
       "      <td>book/1john.txt</td>\n",
       "      <td>4:18: There is no fear in love; but perfect l...</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "      <td>16.358726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2949</td>\n",
       "      <td>book/1samuel.txt</td>\n",
       "      <td>20:17: And Jonathan caused David to swear agai...</td>\n",
       "      <td>537</td>\n",
       "      <td>5</td>\n",
       "      <td>16.358726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14192</td>\n",
       "      <td>book/hosea.txt</td>\n",
       "      <td>3:1: Then said the LORD unto me, Go yet, love ...</td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>16.358726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18898</td>\n",
       "      <td>book/john.txt</td>\n",
       "      <td>13:34: A new commandment I give unto you, That...</td>\n",
       "      <td>622</td>\n",
       "      <td>7</td>\n",
       "      <td>16.358726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18942</td>\n",
       "      <td>book/john.txt</td>\n",
       "      <td>15:9: As the Father hath loved me, so have I l...</td>\n",
       "      <td>666</td>\n",
       "      <td>8</td>\n",
       "      <td>16.358726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22690</td>\n",
       "      <td>book/malachi.txt</td>\n",
       "      <td>1:2: I have loved you, saith the LORD.  Yet ye...</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>16.358726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   docnum          filename  \\\n",
       "0   21820     book/luke.txt   \n",
       "1    1408    book/1john.txt   \n",
       "2    1456    book/1john.txt   \n",
       "3    1462    book/1john.txt   \n",
       "4    1464    book/1john.txt   \n",
       "5    2949  book/1samuel.txt   \n",
       "6   14192    book/hosea.txt   \n",
       "7   18898     book/john.txt   \n",
       "8   18942     book/john.txt   \n",
       "9   22690  book/malachi.txt   \n",
       "\n",
       "                                                line line_num  rank      score  \n",
       "0  6:32: For if ye love them which love you, what...      287     0  21.811635  \n",
       "1   2:15: Love not the world, neither the things ...       27     1  16.358726  \n",
       "2   4:10: Herein is love, not that we loved God, ...       75     2  16.358726  \n",
       "3   4:16: And we have known and believed the love...       81     3  16.358726  \n",
       "4   4:18: There is no fear in love; but perfect l...       83     4  16.358726  \n",
       "5  20:17: And Jonathan caused David to swear agai...      537     5  16.358726  \n",
       "6  3:1: Then said the LORD unto me, Go yet, love ...       37     6  16.358726  \n",
       "7  13:34: A new commandment I give unto you, That...      622     7  16.358726  \n",
       "8  15:9: As the Father hath loved me, so have I l...      666     8  16.358726  \n",
       "9  1:2: I have loved you, saith the LORD.  Yet ye...        4     9  16.358726  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from whoosh.searching import Hit \n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "with ix.searcher(weighting=scoring.TF_IDF()) as s:\n",
    "    qp = QueryParser(\"content\", ix.schema)\n",
    "    user_q = qp.parse(u\"love\")\n",
    "    \n",
    "    results = s.search(user_q)\n",
    "    print(\"Total no of matches: \",len(results))\n",
    "    \n",
    "    rank=[]\n",
    "    docnum=[]\n",
    "    score=[]\n",
    "    filenames=[]\n",
    "    lines=[]\n",
    "    line_num=[]\n",
    "    \n",
    "    for i in np.arange(0,10):\n",
    "        rank.append(results[i].rank)\n",
    "        docnum.append(results[i].docnum)\n",
    "        score.append(results[i].score)\n",
    "        filenames.append(results[i]['filename'])\n",
    "        line_num.append(results[i]['line_num'])\n",
    "        lines.append(results[i]['content'])\n",
    "       \n",
    "    df = pd.DataFrame({'filename' : filenames, 'line_num' : line_num, 'line' : lines, 'docnum' : docnum, \\\n",
    "                            'score' : score, 'rank' : rank})\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line_num above is the actual line number in the text file. docnum should be the index number in the whole indexes we have created."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
