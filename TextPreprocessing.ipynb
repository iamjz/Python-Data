{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing is an important step in the use of unstructured text document for any type of data mining, information retrieval, or text analytics. We'll use NLTK and look at the following concepts:\n",
    "* Stop Words\n",
    "* Stemming\n",
    "* Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint, string, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "Text documents often contain many occurences of the same word. For example, words such as \"a\", \"the\", \"of\", and \"it\" likely occur very frequently. When classifying a document based on the number of times specific words occur in the text document, these words can lead to biases, especially since they are generally common in ALL text documents you might want to classify. \n",
    "\n",
    "As a result, the concept of stop words was invented. \n",
    "\n",
    "https://en.wikipedia.org/wiki/Stop_words\n",
    "\n",
    "These words are the most commonly occurring words and they should be removed during the tokenization process in order to improve subseqeunt text analytics efforts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer = \"word\", lowercase = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myText = \"Hello! My name is Jacky Zhao. I'm an aspiring data scientist. Follow me on twitter @iamdatabear. The is my text analysis practice!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! My name is Jacky Zhao. I'm an aspiring data scientist. Follow me on twitter @iamdatabear. The is my text analysis practice!\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(lowercase = True)\n",
    "cv2 = CountVectorizer(stop_words = \"english\", lowercase = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tk_func1 = cv1.build_analyzer()\n",
    "tk_func2 = cv2.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent = 2, depth = 1, width = 80, compact = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization for 1: \n",
      " ['hello', 'my', 'name', 'is', 'jacky', 'zhao', 'an', 'aspiring', 'data', 'scientist', 'follow', 'me', 'on', 'twitter', 'iamdatabear', 'the', 'is', 'my', 'text', 'analysis', 'practice']\n"
     ]
    }
   ],
   "source": [
    "example1 = tk_func1(myText)\n",
    "print(\"Tokenization for 1: \\n\", example1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization for 2: \n",
      " ['hello', 'jacky', 'zhao', 'aspiring', 'data', 'scientist', 'follow', 'twitter', 'iamdatabear', 'text', 'analysis', 'practice']\n"
     ]
    }
   ],
   "source": [
    "example2 = tk_func2(myText)\n",
    "print(\"Tokenization for 2: \\n\", example2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "We have looked at the removal of redundant or unimportant words (stop words). However, an issue still exist because of different word forms of the same base term. For example: compute, computer, computed, and computing. The process of changing words back to their root term (basic term) so that token frequencies match the use of the root token rather than being spread across multiple tokens is known as stemming\n",
    "\n",
    "https://en.wikipedia.org/wiki/Stemming\n",
    "\n",
    "The most popular stemming is the \"Porter Stemmer\". It was originally published by Martin Porterin 1980. Since then, an improved version was released in 2000. NLTK includes the Porter Stemmer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_words = [\"python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "for w in example_words:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newText = \"It is important to be very pythonly while you are pythoning with pythong. All pythoners have pythoned poorly at least once!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is important to be very pythonly while you are pythoning with pythong. All pythoners have pythoned poorly at least once!'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(newText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'important', 'to', 'be', 'very', 'pythonly', 'while', 'you', 'are', 'pythoning', 'with', 'pythong', '.', 'All', 'pythoners', 'have', 'pythoned', 'poorly', 'at', 'least', 'once', '!']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [token for token in tokens if token not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'important', 'to', 'be', 'very', 'pythonly', 'while', 'you', 'are', 'pythoning', 'with', 'pythong', 'All', 'pythoners', 'have', 'pythoned', 'poorly', 'at', 'least', 'once']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "is\n",
      "import\n",
      "to\n",
      "be\n",
      "veri\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "pythong\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n"
     ]
    }
   ],
   "source": [
    "for w in tokens:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Lemmatization in linguistics is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. By inflected, it means to change the form of a word to express a particular grammatical function or attribute, typically tense, mood, person, number, case, and gender.\n",
    "\n",
    "In computational linguistics, lemmatization is the algorithmic process of determining the lemma for a given word. The process may involve complex tasks such as understanding the context and determining the parts of speech of a word. \n",
    "\n",
    "In many languages, words appear in several inflected forms. For example, the verb \"to walk\" may appear as \"walk\", \"walked\", \"walks\", and \"walking\". The base form \"walk\" is called the lemma of the word. \n",
    "\n",
    "Lemmatization is closely related to stemming. The difference is that a stemmer operates on a single word without the knowledge of the context and therefore cannot discriminate between words which have different meanings depending on the part of speech. However, stemmers are typically easier to implement and run much faster. The reduced accuracy may not matter for some applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"dogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization vs Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = [\"going\", \"gone\", \"goes\", \"went\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: \n",
      "\n",
      "going becomes go\n",
      "gone becomes gone\n",
      "goes becomes goe\n",
      "went becomes went\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemming: \\n\")\n",
    "for w in words:\n",
    "    print(w,\"becomes\", stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatize without context:\n",
      "\n",
      "going becomes going\n",
      "gone becomes gone\n",
      "goes becomes go\n",
      "went becomes went\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemmatize without context:\\n\")\n",
    "for w in words:\n",
    "    print(w, \"becomes\", lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatize WITH context: \n",
      "\n",
      "going becomes go\n",
      "gone becomes go\n",
      "goes becomes go\n",
      "went becomes go\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemmatize WITH context: \\n\")\n",
    "for w in words:\n",
    "    print(w, \"becomes\", lemmatizer.lemmatize(w, pos = \"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the stemming process does not generate a real word but a root form. Other the other side, the lemmatizer generates real words, but without contextual information, it's not able to distinguish between nouns and verbs. Hence, the lemmatization process does not change the word. \n",
    "\n",
    "The context is provided by the POS tag (\"v\" for verb). We cannot specify POS tag everytime in order to lemmatize words in a text. NLTK generates POS tags automatially, using a simple function pos_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is a simple sentence. Let's SeE IF iT cAn fiGuRe tHiS cRaZy sEnTenCe ouT!\""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"This is a simple sentence. Let's SeE IF iT cAn fiGuRe tHiS cRaZy sEnTenCe ouT!\"\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'simple', 'sentence', '.', 'Let', \"'s\", 'SeE', 'IF', 'iT', 'cAn', 'fiGuRe', 'tHiS', 'cRaZy', 'sEnTenCe', 'ouT', '!']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_pos = pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('simple', 'JJ'), ('sentence', 'NN'), ('.', '.'), ('Let', 'VB'), (\"'s\", 'POS'), ('SeE', 'NNP'), ('IF', 'IN'), ('iT', 'JJ'), ('cAn', 'JJ'), ('fiGuRe', 'NN'), ('tHiS', 'NN'), ('cRaZy', 'NN'), ('sEnTenCe', 'VBZ'), ('ouT', 'RP'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tokens_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_and_pos = {}\n",
    "\n",
    "for tp in tokens_pos:\n",
    "    word_and_pos[tp[0]] = tp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_and_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': 'DT', 'is': 'VBZ', 'a': 'DT', 'simple': 'JJ', 'sentence': 'NN', '.': '.', 'Let': 'VB', \"'s\": 'POS', 'SeE': 'NNP', 'IF': 'IN', 'iT': 'JJ', 'cAn': 'JJ', 'fiGuRe': 'NN', 'tHiS': 'NN', 'cRaZy': 'NN', 'sEnTenCe': 'VBZ', 'ouT': 'RP', '!': '.'}\n"
     ]
    }
   ],
   "source": [
    "print(word_and_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for k, v in word_and_pos.items():\n",
    "    print(k, \"becomes\", lemmatizer.lemmatize(k, pos = v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words, stemming, and lemmatization are important pre-processing steps in text analytics applications. You can leverage the off-the-shelf solutions offered by NLTK into your text analysis applications. Additionally, many code libraries and applications that perform more advanced text analyticsal processes incorporate these techniques in them by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stringAction = \"We are meeting\"\n",
    "stringNoun = \"We had a meeting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words = \"english\", lowercase = True)\n",
    "tk_function = cv.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent = 2, depth = 1, width = 80, compact = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizations:\n",
      "\n",
      "'We are meeting':\n",
      "['meeting']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizations:\\n\")\n",
    "print(\"'{}':\".format(stringAction))\n",
    "pp.pprint(tk_function(stringAction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:\n",
      "\n",
      "'We had a meeting':\n",
      "['meeting']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenization:\\n\")\n",
    "print(\"'{}':\".format(stringNoun))\n",
    "pp.pprint(tk_function(stringNoun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are meet\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem(stringAction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we had a meet\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem(stringNoun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'are', 'meeting']\n"
     ]
    }
   ],
   "source": [
    "stringActionTokens = nltk.word_tokenize(stringAction)\n",
    "sat = [t for t in stringActionTokens if t not in string.punctuation]\n",
    "print(sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'had', 'a', 'meeting']\n"
     ]
    }
   ],
   "source": [
    "stringNounTokens = nltk.word_tokenize(stringNoun)\n",
    "snt = [t for t in stringNounTokens if t not in string.punctuation]\n",
    "print(snt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "are\n",
      "meeting\n"
     ]
    }
   ],
   "source": [
    "for w in sat:\n",
    "    print(lem.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "had\n",
      "a\n",
      "meeting\n"
     ]
    }
   ],
   "source": [
    "for w in snt:\n",
    "    print(lem.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('We', 'PRP'), ('are', 'VBP'), ('meeting', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(stringActionTokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('We', 'PRP'), ('had', 'VBD'), ('a', 'DT'), ('meeting', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(stringNounTokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
